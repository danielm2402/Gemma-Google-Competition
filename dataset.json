[
    {
        "id": "Q1",
        "question": "Explain the central limit theorem and give examples of when you can use it in a real-world problem.",
        "answers": [
            "The center limit theorem states that if any random variable, regardless of the distribution, is sampled a large enough time, the sample mean will be approximately normally distributed. This allows for studying the properties of any statistical distribution as long as there is a large enough sample size."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q2",
        "question": "Briefly explain the A/B testing and its application? What are some common pitfalls encountered in A/B testing?",
        "answers": [
            "/B testing helps us to determine whether a change in something will cause a change in performance significantly or not. So in other words you aim to statistically estimate the impact of a given change within your digital product (for example). You measure success and counter metrics on at least 1 treatment vs 1 control group (there can be more than 1 XP group for multivariate tests)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q3",
        "question": "Describe briefly the hypothesis testing and p-value in layman’s term? And give a practical application for them ?",
        "answers": [
            "Hypothesis test is where you have a current state (null hypothesis) and an alternative state (alternative hypothesis). You assess the results of both of the states and see some differences. You want to decide whether the difference is due to the alternative approach or not."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q4",
        "question": "Mention three ways to make your model robust to outliers.",
        "answers": [
            "Investigating the outliers is always the first step in understanding how to treat them. After you understand the nature of why the outliers occurred you can apply one of the several methods mentioned"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q5",
        "question": "What is the meaning of selection bias and how to avoid it?",
        "answers": [
            "Sampling bias is the phenomenon that occurs when a research study design fails to collect a representative sample of a target population. This typically occurs because the selection criteria for respondents failed to capture a wide enough sampling frame to represent all viewpoints."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q6",
        "question": "Explain the long-tailed distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?",
        "answers": [
            "A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q7",
        "question": "What is the meaning of KPI in statistics",
        "answers": [
            "KPI stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. From finance and HR to marketing and sales, key performance indicators help every area of the business move forward at the strategic level. Types of KPIs Key performance indicators come in many flavors. While some are used to measure monthly progress against a goal, others have a longer-term focus. The one thing all KPIs have in common is that they’re tied to strategic goals. Here’s an overview of some of the most common types of KPIs."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q8",
        "question": "Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not?",
        "answers": [
            "The null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased. The p-value is the probability of observing the results obtained given that the null hypothesis is true, in this case, the coin is fair."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q9",
        "question": "Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not?",
        "answers": [
            "The main consideration when we have a large number of tests is that probability of getting a significant test due to chance alone increases. This will increase the type 1 error (rejecting the null hypothesis when it's actually true. Therefore we need to consider the Bonferroni Effect which happens when we make many tests. Ex. If our significance level is 0.05 but we made a 100 test it means that the probability of getting a value inside the rejection rejoin is 0.0005, not 0.05 so here we need to use another significance level which's called alpha star = significance level /K Where K is the number of the tests"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q10",
        "question": "What general conditions must be satisfied for the central limit theorem to hold?",
        "answers": [
            "In order to apply the central limit theorem, there are four conditions that must be met: 1.** Randomization:** The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample. Independence: The sample values must be independent of each other. The 10% Condition: When the sample is drawn without replacement, the sample size should be no larger than 10% of the population. Large Sample Condition: The sample size needs to be sufficiently large."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q11",
        "question": "What is skewness discuss two methods to measure it?",
        "answers": [
            "Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed.Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. There are two main types of skewness negative skew which refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. These two skews refer to the direction or weight of the distribution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q12",
        "question": "You sample from a uniform distribution [0, d] n times. What is your best estimate of d?",
        "answers": [
            "Intuitively it is the maximum of the sample points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q13",
        "question": "Discuss the Chi-square, ANOVA, and t-test",
        "answers": [
            "Chi-square test A statistical method is used to find the difference or correlation between the observed and expected categorical variables in the dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q14",
        "question": "What is the relationship between the significance level and the confidence level in Statistics?",
        "answers": [
            "Confidence level = 1 - significance level. It's closely related to hypothesis testing and confidence intervals. Significance Level according to the hypothesis testing literature means the probability of Type-I error one is willing to tolerate. Confidence Level according to the confidence interval literature means the probability in terms of the true parameter value lying inside the confidence interval. They are usually written in percentages."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q15",
        "question": "What is the Law of Large Numbers in statistics and how it can be used in data science ?",
        "answers": [
            "The law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates. In data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modeling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data. The law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q16",
        "question": "What is the difference between a confidence interval and a prediction interval, and how do you calculate them?",
        "answers": [
            "A confidence interval is a range of values that is likely to contain the true value of a population parameter with a certain level of confidence. It is used to estimate the precision or accuracy of a sample statistic, such as a mean or a proportion, based on a sample from a larger population."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q17",
        "question": "You and your friend are playing a game with a fair coin. The two of you will continue to toss the coin until the sequence HH or TH shows up. If HH shows up first, you win, and if TH shows up first your friend win. What is the probability of you winning the game?",
        "answers": [
            "If T is ever flipped, you cannot then reach HH before your friend reaches TH. Therefore, the probability of you winning this is to flip HH initially. Therefore the sample space will be {HH, HT, TH, TT} and the probability of you winning will be (1/4) and your friend (3/4)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q18",
        "question": "If you roll a dice three times, what is the probability to get two consecutive threes?If you roll a dice three times, what is the probability to get two consecutive threes?",
        "answers": [
            "The right answer is 11/216."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q19",
        "question": "Suppose you have ten fair dice. If you randomly throw them simultaneously, what is the probability that the sum of all of the top faces is divisible by six?",
        "answers": [
            "1/6. Explanation: With 10 dices, the possible sums divisible by 6 are 12, 18, 24, 30, 36, 42, 48, 54, and 60. You don't actually need to calculate the probability of getting each of these numbers as the final sums from 10 dices because no matter what the sum of the first 9 numbers is, you can still choose a number between 1 to 6 on the last die and add to that previous sum to make the final sum divisible by 6. Therefore, we only care about the last die. And the probability to get that number on the last die is 1/6. So the answer is 1/6"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q20",
        "question": "If you have three draws from a uniformly distributed random variable between 0 and 2, what is the probability that the median of three numbers is greater than 1.5?",
        "answers": [
            "The right answer is 5/32 or 0.156. There are different methods to solve it: Method 1: To get a median greater than 1.5 at least two of the three numbers must be greater than 1.5. The probability of one number being greater than 1.5 in this distribution is 0.25. Then, using the binomial distribution with three trials and a success probability of 0.25 we compute the probability of 2 or more successes to get the probability of the median is more than 1.5, which would be about 15.6%. Method2 : A median greater than 1.5 will occur when o all three uniformly distributed random numbers are greater than 1.5 or 1 uniform distributed random number between 0 and 1.5 and the other two are greater than 1.5. So, the probability of the above event is = {(2 - 1.5) / 2}^3 + (3 choose 1)(1.5/2)(0.5/2)^2 = 10/64 = 5/32"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q21",
        "question": "Assume you have a deck of 100 cards with values ranging from 1 to 100 and you draw two cards randomly without replacement, what is the probability that the number of one of them is double the other?",
        "answers": [
            "There are a total of (100 C 2) = 4950 ways to choose two cards at random from the 100 cards and there are only 50 pairs of these 4950 ways that you will get one number and it's double. Therefore the probability that the number of one of them is double the other is 50/4950."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q22",
        "question": "If there are 30 people in a room, what is the probability that everyone has different birthdays?",
        "answers": [
            "The sample space is 365^30 and the number of events is 365p30 because we need to choose persons without replacement to get everyone to have a unique birthday therefore the Prob = 356p30 / 365^30 = 0.2936"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q23",
        "question": "Assume two coins, one fair and the other is unfair. You pick one at random, flip it five times, and observe that it comes up as tails all five times. What is the probability that you are fliping the unfair coin?",
        "answers": [
            "Let's use Baye’s theorem let U denote the case where you are flipping the unfair coin and F denote the case where you are flipping the fair coin. Since the coin is chosen randomly, we know that P(U)=P(F)=0.5. Let 5T denote the event of flipping 5 tails in a row. Then, we are interested in solving for P(U|5T) (the probability that you are flipping the unfair coin given that you obtained 5 tails). Since the unfair coin always results in tails, therefore P(5T|U) = 1 and also P(5T|F) =1/2⁵ = 1/32 by the definition of a fair coin. Lets apply Bayes theorem where P(U|5T) = P(5T|U) * P(U) / P(5T|U)* P(U) + P(5T|F)* P(F) = 0.5 / 0.5 +0.5* 1/32 = 0.97. Therefore the probability that you picked the unfair coin is 97%"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q24",
        "question": "Assume you take a stick of length 1 and you break it uniformly at random into three parts. What is the probability that the three pieces can be used to form a triangle?",
        "answers": [
            "The right answer is 0.25. Let's say, x and y are the lengths of the two parts, so the length of the third part will be 1-x-y. As per the triangle inequality theorem, the sum of two sides should always be greater than the third side. Therefore, no two lengths can be more than 1/2. x<1/2 y<1/2. Based on the triangle inequality theorem: x+y > 1-a-b x+y > 1/2. From the diagram below, there is only one triangle that matches all the above conditions out of 4 triangles. Therefore, the probability will be 1/4"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q25",
        "question": "Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?",
        "answers": [
            "For making 2 chords, 4 points are necessary and from 4 points there are 3 different combinations of pairs of chords can be made. From the 3 combinations, there is only one combination in which the two chords intersect hence answer is 1/3. Let's assume that P1, P2, P3, and P4 are four points then 3 different combinations are possible for pairs of chords: (P1 P2) (P3 P4) or (P1 P3) (P4 P2) or (P1 P4) (P2 P3) there the 3rd one will only intersect."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q26",
        "question": "If there’s a 15% probability that you might see at least one airplane in a five-minute interval, what is the probability that you might see at least one airplane in a period of half an hour?",
        "answers": [
            "Probability of at least one plane in 5 mins interval=0.15 Probability of no plane in 5 mins interval=0.85 Probability of seeing at least one plane in 30 mins=1 - Probability of not seeing any plane in 30 minutes =1-(0.85)^6 = 0.6228"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q27",
        "question": "According to hospital records, 75% of patients suffering from a disease die from that disease. Find out the probability that 4 out of the 6 randomly selected patients survive.",
        "answers": [
            "This has to be a binomial since there are only 2 outcomes – death or life"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q28",
        "question": "You have 40 cards in four colors, 10 reds, 10 greens, 10 blues, and ten yellows. Each color has a number from 1 to 10. When you pick two cards without replacement, what is the probability that the two cards are not in the same color and not in the same number?",
        "answers": [
            "Since it doesn't matter how you choose the first card, so, choose one card at random. Now, all we have to care about is the restriction on the second card. It can't be the same number (i.e. 3 cards from the other colors can't be chosen in favorable cases) and also can't be the same color (i.e. 9 cards from the same color can't be chosen keep in mind we have already picked one). So, the number of favorable choices for the 2nd card is (39-12)/39 = 27/39 = 9/13"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q29",
        "question": "Can you explain the difference between frequentist and Bayesian probability approaches?",
        "answers": [
            "The frequentist approach to probability defines probability as the long-run relative frequency of an event in an infinite number of trials. It views probabilities as fixed and objective, determined by the data at hand. In this approach, the parameters of a model are treated as fixed and unknown and estimated using methods like maximum likelihood estimation."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q30",
        "question": "Explain the Difference Between Probability and Likelihood",
        "answers": [
            "Probability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses. Probability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5. The likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q31",
        "question": "Describe the motivation behind random forests and mention two reasons why they are better than individual decision trees.",
        "answers": [
            "The motivation behind random forest or ensemble models in general in layman's terms, Let's say we have a question/problem to solve we bring 100 people and ask each of them the question/problem and record their solution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q32",
        "question": "What are the differences and similarities between gradient boosting and random forest? and what are the advantages and disadvantages of each when compared to each other?",
        "answers": [
            "Similarities: (a) Both these algorithms are decision-tree-based algorithms. (b) Both these algorithms are ensemble algorithms. (c) Both are flexible models and do not need much data preprocessing."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q33",
        "question": "What are L1 and L2 regularization? What are the differences between the two?",
        "answers": [
            "Regularization is a technique used to avoid overfitting by trying to make the model more simple."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q34",
        "question": "What are the Bias and Variance in a Machine Learning Model and explain the bias-variance trade-off?",
        "answers": [
            "The goal of any supervised machine learning model is to estimate the mapping function (f) that predicts the target variable (y) given input (x)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q35",
        "question": "Mention three ways to handle missing or corrupted data in a dataset",
        "answers": [
            "In general, real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q36",
        "question": "Explain briefly the logistic regression model and state an example of when you have used it recently",
        "answers": [
            "Logistic regression is used to calculate the probability of occurrence of an event in the form of a dependent output variable based on independent input variables. Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. If the probability is bigger than 0.5 then it will belong to that class (positive) and if it is below 0.5 it will belong to the other class. This will make it a binary classifier."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q37",
        "question": "Explain briefly batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. and what are the pros and cons for each of them?",
        "answers": [
            "Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function. Batch Gradient Descent: In Batch Gradient descent the whole training data is used to minimize the loss function by taking a step toward the nearest minimum by calculating the gradient (the direction of descent). Pros: Since the whole data set is used to calculate the gradient it will be stable and reach the minimum of the cost function without bouncing (if the learning rate is chosen cooreclty)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q38",
        "question": "Explain what is information gain and entropy in the context of decision trees.",
        "answers": [
            "Entropy and Information Gain are two key metrics used in determining the relevance of decision-making when constructing a decision tree model and determining the nodes and the best way to split. The idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label. Entropy is the measure of impurity, disorder, or uncertainty in a bunch of examples. Entropy controls how a Decision Tree decides to split the data. Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q39",
        "question": "Explain the linear regression model and discuss its assumption.",
        "answers": [
            "Linear regression is a supervised statistical model to predict dependent variable quantity based on independent variables. Linear regression is a parametric model and the objective of linear regression is that it has to learn coefficients using the training data and predict the target value given only independent values."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q40",
        "question": "Explain briefly the K-Means clustering and how can we find the best value of K?",
        "answers": [
            "K-Means is a well-known clustering algorithm. K-means clustering is often used because it is easy to interpret and implement."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q41",
        "question": "Define Precision, recall, and F1 and discuss the trade-off between them?",
        "answers": [
            "Precision and recall are two classification evaluation metrics that are used beyond accuracy."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q42",
        "question": "What are the differences between a model that minimizes squared error and the one that minimizes the absolute error? and in which cases each error metric would be more appropriate?",
        "answers": [
            "Both mean square error (MSE) and mean absolute error (MAE) measures the distances between vectors and express average model prediction in units of the target variable. Both can range from 0 to infinity, the lower they are the better the model. The main difference between them is that in MSE the errors are squared before being averaged while in MAE they are not. This means that a large weight will be given to large errors. MSE is useful when large errors in the model are trying to be avoided. This means that outliers affect MSE more than MAE, that is why MAE is more robust to outliers. Computation-wise MSE is easier to use as the gradient calculation will be more straightforward than MAE, which requires linear programming to calculate it."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q43",
        "question": "Define and compare parametric and non-parametric models and give two examples for each of them?",
        "answers": [
            "Parametric models assume that the dataset comes from a certain function with some set of parameters that should be tuned to reach the optimal performance. For such models, the number of parameters is determined prior to training, thus the degree of freedom is limited and reduces the chances of overfitting."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q44",
        "question": "Explain the kernel trick in SVM. Why do we use it and how to choose what kernel to use?",
        "answers": [
            "Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q45",
        "question": "Define the cross-validation process and the motivation behind using it",
        "answers": [
            "Cross-validation is a technique used to assess the performance of a learning model in several subsamples of training data. In general, we split the data into train and test sets where we use the training data to train our model and the test data to evaluate the performance of the model on unseen data and validation set for choosing the best hyperparameters. Now, a random split in most cases(for large datasets) is fine. However, for smaller datasets, it is susceptible to loss of important information present in the data in which it was not trained. Hence, cross-validation though computationally expensive combats this issue."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q46",
        "question": "You are building a binary classifier and you found that the data is imbalanced, what should you do to handle this situation?",
        "answers": [
            "If there is a data imbalance there are several measures we can take to train a fairer binary classifier: (a) Pre-Processing: Check whether you can get more data or not. Use sampling techniques (Sample minority class, Downsample majority class, can take the hybrid approach as well). We can also use data augmentation to add more data points for the minority class but with little deviations/changes leading to new data points that are similar to the ones they are derived from. The most common/popular technique is SMOTE (Synthetic Minority Oversampling technique). Suppression: Though not recommended, we can drop off some features directly responsible for the imbalance. Learning Fair Representation: Projecting the training examples to a subspace or plane minimizes the data imbalance. Re-Weighting: We can assign some weights to each training example to reduce the imbalance in the data. (b) In-Processing: Regularisation: We can add score terms that measure the data imbalance in the loss function and therefore minimizing the loss function will also minimize the degree of imbalance concerning the score chosen which also indirectly minimizes other metrics that measure the degree of data imbalance. Adversarial Debiasing: Here we use the adversarial notion to train the model where the discriminator tries to detect if there are signs of data imbalance in the predicted data by the generator and hence the generator learns to generate data that is less prone to imbalance. (c) Post-Processing: Odds-Equalization: Here we try to equalize the odds for the classes with respect to the data is imbalanced for correct imbalance in the trained model. Usually, the F1 score is a good choice, if both precision and recall scores are important. Choose appropriate performance metrics. For example, accuracy is not a correct metric to use when classes are imbalanced. Instead, use precision, recall, F1 score, and ROC curve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q47",
        "question": "You are working on a clustering problem, what are different evaluation metrics that can be used, and how to choose between them?",
        "answers": [
            "Clusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations and similar observations together, then it has performed well. The two most popular metrics evaluation metrics for clustering algorithms are the 𝐒𝐢𝐥𝐡𝐨𝐮𝐞𝐭𝐭𝐞 𝐜𝐨𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭 and 𝐃𝐮𝐧𝐧’𝐬 𝐈𝐧𝐝𝐞𝐱."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q48",
        "question": "What is the ROC curve and when should you use it?",
        "answers": [
            "ROC curve, Receiver Operating Characteristic curve, is a graphical representation of the model's performance where we plot the True Positive Rate (TPR) against the False Positive Rate (FPR) for different threshold values, for hard classification, between 0 to 1 based on model output. This ROC curve is mainly used to compare two or more models as shown in the figure below. Now, it is easy to see that a reasonable model will always give FPR less (since it's an error) than TPR so, the curve hugs the upper left corner of the square box 0 to 1 on the TPR axis and 0 to 1 on the FPR axis."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q49",
        "question": "What is the difference between hard and soft voting classifiers in the context of ensemble learners?",
        "answers": [
            "Hard Voting: We take into account the class predictions for each classifier and then classify an input based on the maximum votes to a particular class. Soft Voting: We take into account the probability predictions for each class by each classifier and then classify an input to the class with maximum probability based on the average probability (averaged over the classifier's probabilities) for that class."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q50",
        "question": "What is boosting in the context of ensemble learners discuss two famous boosting methods",
        "answers": [
            "Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q51",
        "question": "How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?",
        "answers": [
            "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q52",
        "question": "Define the curse of dimensionality and how to solve it.",
        "answers": [
            "Curse of dimensionality represents the situation when the amount of data is too few to be represented in a high-dimensional space, as it will be highly scattered in that high-dimensional space and it becomes more probable that we overfit this data. If we increase the number of features, we are implicitly increasing model complexity and if we increase model complexity we need more data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q53",
        "question": "In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?",
        "answers": [
            "Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q54",
        "question": "Discuss two clustering algorithms that can scale to large datasets",
        "answers": [
            "Minibatch Kmeans: Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)  is a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q55",
        "question": "Do you need to scale your data if you will be using the SVM classifier and discus your answer",
        "answers": [
            "Yes, feature scaling is required for SVM and all margin-based classifiers since the optimal hyperplane (the decision boundary) is dependent on the scale of the input features. In other words, the distance between two observations will differ for scaled and non-scaled cases, leading to different models being generated. This can be seen in the figure below, when the features have different scales, we can see that the decision boundary and the support vectors are only classifying the X1 features without taking into consideration the X0 feature, however after scaling the data to the same scale the decision boundaries and support vectors are looking much better and the model is taking into account both features."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q56",
        "question": "What are Loss Functions and Cost Functions? Explain the key Difference Between them.",
        "answers": [
            "The loss function is the measure of the performance of the model on a single training example, whereas the cost function is the average loss function over all training examples or across the batch in the case of mini-batch gradient descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q57",
        "question": "What is the importance of batch in machine learning and explain some batch-dependent gradient descent algorithms?",
        "answers": [
            "In the memory, the dataset can load either completely at once or in the form of a set. If we have a huge size of the dataset, then loading the whole data into memory will reduce the training speed, hence batch term is introduced."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q58",
        "question": "What are the different methods to split a tree in a decision tree algorithm?",
        "answers": [
            "Decision trees can be of two types regression and classification. For classification, classification accuracy created a lot of instability."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q59",
        "question": "Why boosting is a more stable algorithm as compared to other ensemble algorithms?",
        "answers": [
            "Boosting algorithms focus on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. That’s why boosting is a more stable algorithm compared to other ensemble algorithms."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q60",
        "question": "What is active learning and discuss one strategy of it?",
        "answers": [
            "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes referred to as optimal experimental design."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q61",
        "question": "What are the different approaches to implementing recommendation systems?",
        "answers": [
            "(a) 𝐂𝐨𝐧𝐭𝐞𝐧𝐭-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: Content-Based Filtering depends on similarities of items and users' past activities on the website to recommend any product or service. (b) 𝐂𝐨𝐥𝐥𝐚𝐛𝐨𝐫𝐚𝐭𝐢𝐯𝐞-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: The primary job of a collaborative filtering system is to overcome the shortcomings of content-based filtering. (c) 𝐇𝐲𝐛𝐫𝐢𝐝 𝐟𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: A mixture of content and collaborative methods. Uses descriptors and interactions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q62",
        "question": "What are the evaluation metrics that can be used for multi-label classification?",
        "answers": [
            "Multi-label classification is a type of classification problem where each instance can be assigned to multiple classes or labels simultaneously. The evaluation metrics for multi-label classification are designed to measure the performance of a multi-label classifier in predicting the correct set of labels for each instance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q63",
        "question": "What is the difference between concept and data drift and how to overcome each of them?",
        "answers": [
            "Concept drift and data drift are two different types of problems that can occur in machine learning systems. Concept drift refers to changes in the underlying relationships between the input data and the target variable over time. This means that the distribution of the data that the model was trained on no longer matches the distribution of the data it is being tested on. For example, a spam filter model that was trained on emails from several years ago may not be as effective at identifying spam emails from today because the language and tactics used in spam emails may have changed."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q64",
        "question": "Can you explain the ARIMA model and its components?",
        "answers": [
            "The ARIMA model, which stands for Autoregressive Integrated Moving Average, is a widely used time series forecasting model. It combines three key components: Autoregression (AR), Differencing (I), and Moving Average (MA)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q65",
        "question": "What are the assumptions made by the ARIMA model?",
        "answers": [
            "The ARIMA model makes several assumptions about the underlying time series data. These assumptions are important to ensure the validity and accuracy of the model's results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q66",
        "question": "What is supervised machine learning?",
        "answers": [
            "Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q67",
        "question": "What is regression? Which models can you use to solve a regression problem?",
        "answers": [
            "Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor). Here are some common regression models"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q68",
        "question": "What is linear regression? When do we use it?",
        "answers": [
            "Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q69",
        "question": "What are the main assumptions of linear regression?",
        "answers": [
            "There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q70",
        "question": "What’s the normal distribution? Why do we care about it?",
        "answers": [
            "The normal distribution derives its importance from the Central Limit Theorem, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e the distribution of the mean of the samples is normal. It is important that each sample is independent from the other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q71",
        "question": "How do we check if a variable follows the normal distribution?",
        "answers": [
            "(a) Plot a histogram out of the sampled data. If you can fit the bell-shaped normal curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected. (b) Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution. (c) Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously. (d) Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q72",
        "question": "What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices?",
        "answers": [
            "Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q73",
        "question": "What methods for solving linear regression do you know?",
        "answers": [
            "To solve linear regression, you need to find the coefficients which minimize the sum of squared errors."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q74",
        "question": "What is gradient descent? How does it work?",
        "answers": [
            "Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q75",
        "question": "What is the normal equation?",
        "answers": [
            "Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q76",
        "question": "What is SGD—stochastic gradient descent? What’s the difference with the usual gradient descent?",
        "answers": [
            "In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function. The difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q77",
        "question": "Which metrics for evaluating regression models do you know?",
        "answers": [
            "(1) Mean Squared Error(MSE) (2) Root Mean Squared Error(RMSE) (3) Mean Absolute Error(MAE) (4) R² or Coefficient of Determination (5) Adjusted R²"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q78",
        "question": "What are MSE and RMSE?",
        "answers": [
            "MSE stands for Mean Square Error while RMSE stands for Root Mean Square Error. They are metrics with which we can evaluate models."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q79",
        "question": "What is the bias-variance trade-off?",
        "answers": [
            "Bias is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. Variance is a model sensitivity to changes in the training dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q80",
        "question": "What is overfitting?",
        "answers": [
            "When your model perform very well on your training set but can't generalize the test set, because it adjusted a lot to the training set."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q81",
        "question": "How to validate your models?",
        "answers": [
            "One of the most common approaches is splitting data into train, validation and test parts. Models are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset. Another approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds. Also you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q82",
        "question": "Why do we need to split our data into three parts: train, validation, and test?",
        "answers": [
            "The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never seen before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q83",
        "question": "Can you explain how cross-validation works?",
        "answers": [
            "Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q84",
        "question": "What is K-fold cross-validation?",
        "answers": [
            "K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set. It should not be used in a time series data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q85",
        "question": "How do we choose K in K-fold cross-validation? What’s your favorite K?",
        "answers": [
            "There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. I tend to use 4 for small datasets and 5 for large ones as K."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q86",
        "question": "What is classification? Which models would you use to solve a classification problem?",
        "answers": [
            "Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q87",
        "question": "What is logistic regression? When do we need to use it?",
        "answers": [
            "Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, spam and not spam, churn and not churn and so on. The variable is said to be a binary or dichotomous."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q88",
        "question": "Is logistic regression a linear model? Why?",
        "answers": [
            "Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q89",
        "question": "What is sigmoid? What does it do?",
        "answers": [
            "A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q90",
        "question": "How do we evaluate classification models?",
        "answers": [
            "Depending on the classification problem, we can use the following evaluation metrics: (1) Accuracy (2) Precision (3) Recall (4) F1 Score (5) Logistic loss (also known as Cross-entropy loss) (6) Jaccard similarity coefficient score"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q91",
        "question": "What is accuracy?",
        "answers": [
            "Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q92",
        "question": "Is accuracy always a good metric?",
        "answers": [
            "Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q93",
        "question": "What is the confusion table? What are the cells in this table?",
        "answers": [
            "Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q94",
        "question": "What are precision, recall, and F1-score?",
        "answers": [
            "Precision and recall are classification evaluation metrics: (1) P = TP / (TP + FP) and R = TP / (TP + FN). (2) Where TP is true positives, FP is false positives and FN is false negatives (3) In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives. (4) F1 is a combination of both precision and recall in one score (harmonic mean): (5) F1 = 2 * PR / (P + R). (6) Max F score is 1 and min is 0, with 1 being the best."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q95",
        "question": "Precision-recall trade-off",
        "answers": [
            "Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. In an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q96",
        "question": "What is the ROC curve? When to use it?",
        "answers": [
            "ROC stands for Receiver Operating Characteristics. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q97",
        "question": "What is AUC (AU ROC)? When to use it?",
        "answers": [
            "AUC stands for Area Under the ROC Curve. ROC is a probability curve and AUC represents degree or measure of separability. It's used when we need to value how much model is capable of distinguishing between classes. The value is between 0 and 1, the higher the better."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q98",
        "question": "How to interpret the AU ROC score?",
        "answers": [
            "AUC score is the value of Area Under the ROC Curve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q99",
        "question": "What is the PR (precision-recall) curve?",
        "answers": [
            "A precision-recall curve (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q100",
        "question": "What is the area under the PR curve? Is it a useful metric?",
        "answers": [
            "The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q101",
        "question": "In which cases AU PR is better than AU ROC?",
        "answers": [
            "What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR. Typically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q102",
        "question": "What do we do with categorical variables?",
        "answers": [
            "Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including: One-hot encoding. Label encoding. Ordinal encoding. Target encoding"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q103",
        "question": "Why do we need one-hot encoding?",
        "answers": [
            "If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q104",
        "question": "What is curse of dimensionality?",
        "answers": [
            "The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that the curse of dimensionality is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q105",
        "question": "What happens to our linear regression model if we have three columns in our data: x, y, z  —  and z is a sum of x and y?",
        "answers": [
            "We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression  would be a singular (not invertible) matrix."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q106",
        "question": "What is regularization? Why do we need it?",
        "answers": [
            "Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q107",
        "question": "Which regularization techniques do you know?",
        "answers": [
            "There are mainly two types of regularization, L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. Where  determines the amount of regularization."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q108",
        "question": "What kind of regularization techniques are applicable to linear models?",
        "answers": [
            "AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin–Osher–Fatemi model (TV), Potts model, RLAD, Dantzig Selector,SLOPE"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q109",
        "question": "How does L2 regularization look like in a linear model?",
        "answers": [
            "L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q110",
        "question": "How do we select the right regularization parameters?",
        "answers": [
            "Regularization parameters can be chosen using a grid search"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q111",
        "question": "What’s the effect of L2 regularization on the weights of a linear model?",
        "answers": [
            "L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q112",
        "question": "How L1 regularization looks like in a linear model?",
        "answers": [
            "L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q113",
        "question": "What’s the difference between L2 and L1 regularization?",
        "answers": [
            "Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared. Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not. Computational efficiency: L2 has an analytical solution, while L1 does not. Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q114",
        "question": "Can we have both L1 and L2 regularization components in a linear model?",
        "answers": [
            "Yes, elastic net regularization combines L1 and L2 regularization."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q115",
        "question": "What’s the interpretation of the bias term in linear models?",
        "answers": [
            "Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q116",
        "question": "How do we interpret weights in linear models?",
        "answers": [
            "Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds. If the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q117",
        "question": "If a weight for one variable is higher than for another  —  can we say that this variable is more important?",
        "answers": [
            "Yes - if your predictor variables are normalized. Without normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q118",
        "question": "When do we need to perform feature normalization for linear models? When it’s okay not to do it?",
        "answers": [
            "Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. Linear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, — it adds the regularization matrix to the feature matrix before inverting it."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q119",
        "question": "What is feature selection? Why do we need it?",
        "answers": [
            "Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q120",
        "question": "Is feature selection important for linear models?",
        "answers": [
            "Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q121",
        "question": "Which feature selection techniques do you know?",
        "answers": [
            "Here are some of the feature selections: Principal Component Analysis. Neighborhood Component Analysis. ReliefF Algorithm"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q122",
        "question": "Can we use L1 regularization for feature selection?",
        "answers": [
            "Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q123",
        "question": "Can we use L2 regularization for feature selection?",
        "answers": [
            "No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q124",
        "question": "What are the decision trees?",
        "answers": [
            "This is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. A decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable. Various techniques : like Gini, Information Gain, Chi-square, entropy."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q125",
        "question": "How do we train decision trees?",
        "answers": [
            "Start at the root node. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q126",
        "question": "What are the main parameters of the decision tree model?",
        "answers": [
            "(1) maximum tree depth (2) inimum samples per leaf node (3) impurity criterion"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q127",
        "question": "How do we handle categorical variables in decision trees?",
        "answers": [
            "Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q128",
        "question": "What are the benefits of a single decision tree compared to more complex models?",
        "answers": [
            "(1) easy to implement (2) fast training (3) fast inference (4) good explainability"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q129",
        "question": "How can we know which features are more important for the decision tree model?",
        "answers": [
            "Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q130",
        "question": "What is random forest?",
        "answers": [
            "Random Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q131",
        "question": "Why do we need randomization in random forest?",
        "answers": [
            "Random forest in an extension of the bagging algorithm which takes random data samples from the training dataset (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a random sample of m features from full set of n features (with replacement) and uses this subset of features as candidates for the split"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q132",
        "question": "What are the main parameters of the random forest model?",
        "answers": [
            "max_depth: Longest Path between root node and the leaf, min_sample_split: The minimum number of observations needed to split a given node, max_leaf_nodes: Conditions the splitting of the tree and hence, limits the growth of the trees, min_samples_leaf: minimum number of samples in the leaf node, n_estima ors: Number of trees, max_sample: Fraction of original dataset given to any individual tree in the given model, max_features: Limits the maximum number of features provided to trees in random forest mode"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q133",
        "question": "How do we select the depth of the trees in random forest?",
        "answers": [
            "The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting: limit the maximum depth of a tree. limit the number of test nodes. limit the minimum number of objects at a node required to split. do not split a node when, at least, one of the resulting subsample sizes is below a given threshold. stop developing a node if it does not sufficiently improve the fit."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q134",
        "question": "How do we know how many trees we need in random forest?",
        "answers": [
            "The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q135",
        "question": "Is it easy to parallelize training of a random forest model? How can we do it?",
        "answers": [
            "Yes, R provides a simple way to parallelize training of random forests on large scale data. It makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q136",
        "question": "What are the potential problems with many large trees?",
        "answers": [
            "(1) Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn't generalize well to new, unseen data. (2) Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset. (3) Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware. (4) Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important. (5) Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive. It's important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q137",
        "question": "What happens when we have correlated features in our data?",
        "answers": [
            "In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. In general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to do the same job i.e. explain some variance, reduce entropy, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q138",
        "question": "What is gradient boosting trees?",
        "answers": [
            "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q139",
        "question": "What s the difference between random forest and gradient boosting?",
        "answers": [
            "1 Random Forests builds each tree independently while Gradient Boosting builds one tree at a time. 2 Random Forests combine results at the end of the process (by averaging or majority rules) while Gradient Boosting combines results along the way."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q140",
        "question": "Is it possible to parallelize training of a gradient boosting model? How to do it?",
        "answers": [
            "Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost tree_method = 'gpu_hist' option makes training faster by use of GPUs."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q141",
        "question": "What are the main parameters in the gradient boosting model?",
        "answers": [
            "There are many parameters, but below are a few key defaults. 1 learning_rate=0.1 (shrinkage). 2 n_estimators=100 (number of trees). 3 max_depth=3. 4 min_samples_split=2. 5 min_samples_leaf=1. 6 subsample=1.0."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q142",
        "question": "How do you approach tuning parameters in XGBoost or LightGBM?",
        "answers": [
            "Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q143",
        "question": "How do you select the number of trees in the gradient boosting model?",
        "answers": [
            "Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q144",
        "question": "Which hyper-parameter tuning strategies (in general) do you know?",
        "answers": [
            "There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following: 1 Grid Search is an exhaustive approach such that for each hyper-parameter, the user needs to manually give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it's easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge. 2 Random Search is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored. 3 In a completely different framework, Bayesian Optimization is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a simple explanation is that a much simpler/faster acquisition function intelligently chooses (using a surrogate function such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q145",
        "question": "What kind of problems neural nets can solve?",
        "answers": [
            "Neural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q146",
        "question": "How does a usual fully-connected feed-forward neural network work?",
        "answers": [
            "In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q147",
        "question": "Why do we need activation functions?",
        "answers": [
            "The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q148",
        "question": "What are the problems with sigmoid as an activation function?",
        "answers": [
            "The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient — during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q149",
        "question": "What is ReLU? How is it better than sigmoid or tanh?",
        "answers": [
            "ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q150",
        "question": "How we can initialize the weights of a neural network?",
        "answers": [
            "Proper initialization of weight matrix in neural network is very necessary. Simply we can say there are two ways for initializations. 1 Initializing weights with zeroes. Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different. 2 Initializing weights randomly. Assigning random values to weights is better than just 0 assignment. a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time. b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q151",
        "question": "What if we set all the weights of a neural network to 0?",
        "answers": [
            "If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q152",
        "question": "What regularization techniques for neural nets do you know?",
        "answers": [
            "L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. Data Augmentation - This requires some fake data to be created as a part of training set. Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q153",
        "question": "What is dropout? Why is it useful? How does it work?",
        "answers": [
            "Dropout is a technique that at each training step turns off each neuron with a certain probability of p. This way at each iteration we train only 1-p of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter p."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q154",
        "question": "What is backpropagation? How does it work? Why do we need it?",
        "answers": [
            "The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. The weights that minimize the error function is then considered to be a solution to the learning problem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q155",
        "question": "Which optimization techniques for training neural nets do you know?",
        "answers": [
            "Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent(best among gradient descents), Nesterov Accelerated Gradient, Momentum, Adagrad, AdaDelta, Adam(best one. less time, more efficient)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q156",
        "question": "How do we use SGD (stochastic gradient descent) for training a neural net?",
        "answers": [
            "SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q157",
        "question": "What is the learning rate?",
        "answers": [
            "The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the step width during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q158",
        "question": "What happens when the learning rate is too large? Too small?",
        "answers": [
            "A large learning rate can accelerate the training. However, it is possible that we shoot too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won't update even if it is not the best possible global solution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q159",
        "question": "How to set the learning rate?",
        "answers": [
            "There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn't overshoot or converge too slowly."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q160",
        "question": "What is Adam? What’s the main difference between Adam and SGD?",
        "answers": [
            "Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. Adam tends to converge faster, while SGD often converges to more optimal solutions. SGD's high variance disadvantages gets rectified by Adam (as advantage for Adam)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q161",
        "question": "When would you use Adam and when SGD?",
        "answers": [
            "Adam tends to converge faster, while SGD often converges to more optimal solutions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q162",
        "question": "Do we want to have a constant learning rate or we better change it throughout training?",
        "answers": [
            "Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don't want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use a function called StepLR which decays the learning rate of each parameter by value gamma-which we have to pass through argument- after n number of epoch which you can also set through function argument named epoch_size."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q163",
        "question": "How do we decide when to stop training a neural net?",
        "answers": [
            "Simply stop training when the validation error is the minimum."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q164",
        "question": "What is model checkpointing?",
        "answers": [
            "Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q165",
        "question": "How we can use neural nets for computer vision?",
        "answers": [
            "Neural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q166",
        "question": "What’s a convolutional layer?",
        "answers": [
            "The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks’ kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q167",
        "question": "Why do we actually need convolutions? Can’t we use fully-connected layers for that?",
        "answers": [
            "A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q168",
        "question": "What’s pooling in CNN? Why do we need it?",
        "answers": [
            "Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q169",
        "question": "How does max pooling work? Are there other pooling techniques?",
        "answers": [
            "Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q170",
        "question": "What are augmentations? Why do we need them?",
        "answers": [
            "Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q171",
        "question": "What kind of augmentations do you know?",
        "answers": [
            "There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q172",
        "question": "How to choose which augmentations to use?",
        "answers": [
            "Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q173",
        "question": "What is transfer learning? How does it work?",
        "answers": [
            "Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ≠ D_T,or T_S ≠ T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks. In the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q174",
        "question": "What is object detection? Do you know any architectures for that?",
        "answers": [
            "Object detection is finding Bounding Boxes around objects in an image. Architectures : YOLO, Faster RCNN, Center Net"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q175",
        "question": "What is object segmentation? Do you know any architectures for that?",
        "answers": [
            "Object Segmentation is predicting masks. It does not differentiate objects. Architectures : Mask RCNN, UNet"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q176",
        "question": "How can we use machine learning for text classification?",
        "answers": [
            "Machine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q177",
        "question": "What is bag of words? How we can use it for text classification?",
        "answers": [
            "Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q178",
        "question": "What are the advantages and disadvantages of bag of words?",
        "answers": [
            "Advantages: 1 Simple to understand and implement. Disadvantages: 1 The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations. 2 Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons. 3 Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q179",
        "question": "What are N-grams? How can we use them?",
        "answers": [
            "The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q180",
        "question": "What is TF-IDF? How is it useful for text classification?",
        "answers": [
            "Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like “the” that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q181",
        "question": "Which model would you use for text classification with bag of words features?",
        "answers": [
            "1 Bag Of Words model 2 Word2Vec Embeddings 3 fastText Embeddingsn 4 Convolutional Neural Networks (CNN) 5 Long Short-Term Memory (LSTM) 6 Bidirectional Encoder Representations from Transformers (BERT)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q182",
        "question": "Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words?",
        "answers": [
            "Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q183",
        "question": "What are word embeddings? Why are they useful? Do you know Word2Vec?",
        "answers": [
            "Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations. Word Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q184",
        "question": "If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it?",
        "answers": [
            "Approaches ranked from simple to more complex: 1 Take an average over all words 2 Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf). 3 Use ML model like LSTM or Transformer."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q185",
        "question": "What is unsupervised learning?",
        "answers": [
            "Unsupervised learning aims to detect patterns in data where no labels are given."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q186",
        "question": "What is clustering? When do we need it?",
        "answers": [
            "Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q187",
        "question": "Do you know how K-means works?",
        "answers": [
            "1 Partition points into k subsets. 2 Compute the seed points as the new centroids of the clusters of the current partitioning. 3 Assign each point to the cluster with the nearest seed point. 4 Go back to step 2 or stop when the assignment does not change."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q188",
        "question": "How to select K for K-means?",
        "answers": [
            "Domain knowledge, i.e. an expert knows the value of k Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters. Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q189",
        "question": "What are the other clustering algorithms do you know?",
        "answers": [
            "k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise. Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster. DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster. Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q190",
        "question": "Do you know how DBScan works?",
        "answers": [
            "Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood) Cluster defined as maximum set of density-connected points. Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts. p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1. p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q191",
        "question": "When would you choose K-means and when DBScan?",
        "answers": [
            "DBScan is more robust to noise. DBScan is better when the amount of clusters is difficult to guess. K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q192",
        "question": "What is the curse of dimensionality? Why do we care about it?",
        "answers": [
            "Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q193",
        "question": "Do you know any dimensionality reduction techniques?",
        "answers": [
            "Singular Value Decomposition (SVD), Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), T-distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders, Fourier and Wavelet Transforms"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q194",
        "question": "What’s singular value decomposition? How is it typically used for machine learning?",
        "answers": [
            "Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Σ (diagonal matrix) and R^T (right singular values). For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive. Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q195",
        "question": "What is precision and recall at k?",
        "answers": [
            "Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first k results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top k results out of all correct answers for a given query."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q196",
        "question": "What is a recommender system?",
        "answers": [
            "Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q197",
        "question": "What are good baselines when building a recommender system?",
        "answers": [
            "A good recommer system should give relevant and personalized information. It should not recommend items the user knows well or finds easily. It should make diverse suggestions. A user should explore new items."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q198",
        "question": "What is collaborative filtering?",
        "answers": [
            "Collaborative filtering is the most prominent approach to generate recommendations. It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others. A recommendation is calculated as the average of other experiences. Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi. In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating. Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases. Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q199",
        "question": "How we can incorporate implicit feedback (clicks, etc) into our recommender systems?",
        "answers": [
            "In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q200",
        "question": "What is the cold start problem?",
        "answers": [
            "Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q201",
        "question": "Possible approaches to solving the cold start problem?",
        "answers": [
            "Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items. Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q202",
        "question": "What is a time series?",
        "answers": [
            "A time series is a set of observations ordered in time usually collected at regular intervals."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q203",
        "question": "How is time series different from the usual regression problem?",
        "answers": [
            "The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q204",
        "question": "Which models do you know for solving time series problems?",
        "answers": [
            "1 Simple Exponential Smoothing: approximate the time series with an exponential function 2 Trend-Corrected Exponential Smoothing (Holt‘s Method): exponential smoothing that also models the trend 3 Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‘s Method): exponential smoothing that also models trend and seasonality 4 Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component 5 Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables. 6 Deep learning approaches (RNN, LSTM, etc.)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q205",
        "question": "If there’s a trend in our series, how we can remove it? And why would we want to do it?",
        "answers": [
            "We can explicitly model the trend (and/or seasonality) with approaches such as Holt's Method or Holt-Winter's Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ]."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q206",
        "question": "You have a series with only one variable “y” measured at time t. How do predict “y” at time t+1? Which approaches would you use?",
        "answers": [
            "We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q207",
        "question": "You have a series with a variable “y” and a set of features. How do you predict “y” at t+1? Which approaches would you use?",
        "answers": [
            "Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q208",
        "question": "What are the problems with using trees for solving time series problems?",
        "answers": [
            "Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q209",
        "question": "Why do you use feature selection?",
        "answers": [
            "Feature selection is the process of selecting a subset of relevant features for use in model construction. Feature selection is itself useful, but it mostly acts as a filter, muting out features that aren’t useful in addition to your existing features. Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data. Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model. Fewer attributes is desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q210",
        "question": "Explain what regularization is and why it is useful",
        "answers": [
            "Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but can in actuality can be any norm. The model predictions should then minimize the mean of the loss function calculated on the regularized training set."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q211",
        "question": "What’s the difference between L1 and L2 regularization?",
        "answers": [
            "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q212",
        "question": "How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?",
        "answers": [
            "Proposed methods for model validation: If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy. If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data. Use the model for prediction by feeding it new data, and use the coefficient of determination (R squared) as a model validity measure. Use data splitting to form a separate dataset for estimating model parameters, and another for validating predictions. Use jackknife resampling if the dataset contains a small number of instances, and measure validity with R squared and mean squared error (MSE)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q213",
        "question": "Explain what precision and recall are. How do they relate to the ROC curve?",
        "answers": [
            "Calculating precision and recall is actually quite easy. Imagine there are 100 positive cases among 10,000 cases. You want to predict which ones are positive, and you pick 200 to have a better chance of catching many of the 100 positive cases. You record the IDs of your predictions, and when you get the actual results you sum up how many times you were right or wrong."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q214",
        "question": "Is it better to have too many false positives, or too many false negatives?",
        "answers": [
            "It depends on the question as well as on the domain for which we are trying to solve the question. In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive. For spam filtering, a false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task. So, we prefer too many false negatives over many false positives."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q215",
        "question": "How do you deal with unbalanced binary classification?",
        "answers": [
            "Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. For example, you may have a 2-class (binary) classification problem with 100 instances (rows)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q216",
        "question": "What is statistical power?",
        "answers": [
            "Statistical power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true—that is, the ability of a test to detect an effect, if the effect actually exists. To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q217",
        "question": "What are bias and variance, and what are their relation to modeling data?",
        "answers": [
            "Bias is how far removed a model's predictions are from correctness, while variance is the degree to which these predictions vary between model iterations. Bias is generally the distance between the model that you build on the training data (the best model that your model space can provide) and the “real model” (which generates data)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q218",
        "question": "What if the classes are imbalanced? What if there are more than 2 groups?",
        "answers": [
            "Binary classification involves classifying the data into two groups, e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q219",
        "question": "What are some ways I can make my model more robust to outliers?",
        "answers": [
            "There are several ways to make a model more robust to outliers, from different points of view (data preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human’s knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q220",
        "question": "In unsupervised learning, if a ground truth about a dataset is unknown, how can we determine the most useful number of clusters to be?",
        "answers": [
            "The elbow method is often the best place to start, and is especially useful due to its ease of explanation and verification via visualization. The elbow method is interested in explaining variance as a function of cluster numbers (the k in k-means). By plotting the percentage of variance explained against k, the first N clusters should add significant information, explaining variance; yet, some eventual value of k will result in a much less significant gain in information, and it is at this point that the graph will provide a noticeable angle. This angle will be the optimal number of clusters, from the perspective of the elbow method, It should be self-evident that, in order to plot this variance against varying numbers of clusters, varying numbers of clusters must be tested. Successive complete iterations of the clustering method must be undertaken, after which the results can be plotted and compared. DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q221",
        "question": "Define variance",
        "answers": [
            "Variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself. Var(X) = E[(X - m)^2], m=E[X] Variance is, thus, a measure of the scatter of the values of a random variable relative to its mathematical expectation."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q222",
        "question": "Expected value",
        "answers": [
            "Expected value — Expected Value (Probability Distribution In a probability distribution, expected value is the value that a random variable takes with greatest likelihood. Based on the law of distribution of a random variable x, we know that a random variable x can take values x1, x2, ..., xk with probabilities p1, p2, ..., pk. The mathematical expectation M(x) of a random variable x is equal. The mathematical expectation of a random variable X (denoted by M (X) or less often E (X)) characterizes the average value of a random variable (discrete or continuous). Mathematical expectation is the first initial moment of a given CB."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q223",
        "question": "Describe the differences between and use cases for box plots and histograms",
        "answers": [
            "A histogram is a type of bar chart that graphically displays the frequencies of a data set. Similar to a bar chart, a histogram plots the frequency, or raw count, on the Y-axis (vertical) and the variable being measured on the X-axis (horizontal). The only difference between a histogram and a bar chart is that a histogram displays frequencies for a group of data, rather than an individual data point; therefore, no spaces are present between the bars. Typically, a histogram groups data into small chunks (four to eight values per bar on the horizontal axis), unless the range of data is so great that it easier to identify general distribution trends with larger groupings."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q224",
        "question": "How would you find an anomaly in a distribution?",
        "answers": [
            "Best steps to prevent anomalies is to implement policies or checks that can catch them during the data collection stage. Unfortunately, you do not often get to collect your own data, and often the data you're mining was collected for another purpose. About 68% of all the data points are within one standard deviation from the mean. About 95% of the data points are within two standard deviations from the mean. Finally, over 99% of the data is within three standard deviations from the mean. When the value deviate too much from the mean, let’s say by ± 4σ, then we can considerate this almost impossible value as anomaly. (This limit can also be calculated using the percentile)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q225",
        "question": "How do you deal with outliers in your data?",
        "answers": [
            "For the most part, if your data is affected by these extreme cases, you can bound the input to a historical representative of your data that excludes outliers. So that could be a number of items (>3) or a lower or upper bounds on your order value. If the outliers are from a data set that is relatively unique then analyze them for your specific situation. Analyze both with and without them, and perhaps with a replacement alternative, if you have a reason for one, and report your results of this assessment. One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q226",
        "question": "How do you deal with sparse data?",
        "answers": [
            "We could take a look at L1 regularization since it best fits to the sparse data and do feature selection. If linear relationship - linear regression either - svm. Also it would be nice to use one-hot-encoding or bag-of-words. A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q227",
        "question": "Big Data Engineer Can you explain what REST is?",
        "answers": [
            "REST stands for Representational State Transfer. (It is sometimes spelled ReST.) It relies on a stateless, client-server, cacheable communications protocol -- and in virtually all cases, the HTTP protocol is used. REST is an architecture style for designing networked applications."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q228",
        "question": "Logistic regression",
        "answers": [
            "Log odds - raw output from the model; odds - exponent from the output of the model. Probability of the output - odds / (1+odds)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q229",
        "question": "What is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?",
        "answers": [
            "When predictor variables are correlated, the estimated regression coefficient of any one variable depends on which other predictor variables are included in the model. When predictor variables are correlated, the precision of the estimated regression coefficients decreases as more predictor variables are added to the model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q230",
        "question": "What’s the difference between Gaussian Mixture Model and K-Means?",
        "answers": [
            "Let's says we are aiming to break them into three clusters. K-means will start with the assumption that a given data point belongs to one cluster. Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a red cluster. In the next iteration, we might revise that belief, and be certain that it belongs to the green cluster. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the hard assignment."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q231",
        "question": "Describe how Gradient Boosting works",
        "answers": [
            "The idea of boosting came out of the idea of whether a weak learner can be modified to become better. Gradient boosting relies on regression trees (even when solving a classification problem) which minimize MSE. Selecting a prediction for a leaf region is simple: to minimize MSE we should select an average target value over samples in the leaf. The tree is built greedily starting from the root: for each leaf a split is selected to minimize MSE for this step."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q232",
        "question": "Difference between AdaBoost and XGBoost",
        "answers": [
            "Both methods combine weak learners into one strong learner. For example, one decision tree is a weak learner, and an emsemble of them would be a random forest model, which is a strong learner. Both methods in the learning process will increase the ensemble of weak-trainers, adding new weak learners to the ensemble at each training iteration, i.e. in the case of the forest, the forest will grow with new trees. The only difference between AdaBoost and XGBoost is how the ensemble is replenished."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q233",
        "question": "Data Mining Describe the decision tree model",
        "answers": [
            "A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q234",
        "question": "What is a neural network?",
        "answers": [
            "Neural networks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output as shown in the graphic below."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q235",
        "question": "How do you deal with sparse data?",
        "answers": [
            "We could take a look at L1 regularization since it best fits the sparse data and does feature selection. If linear relationship - linear regression either - svm. Also it would be nice to use one-hot-encoding or bag-of-words. A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q236",
        "question": "Pseudo Labeling",
        "answers": [
            "Pseudo-labeling is a technique that allows you to use predicted with confidence test data in your training process. This effectivey works by allowing your model to look at more samples, possibly varying in distributions. I have found this Kaggle kernel to be useful in understanding how one can use pseudo-labeling in light of having too few train data points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q237",
        "question": "Knowledge Distillation",
        "answers": [
            "It is the process by which a considerably larger model is able to transfer its knowledge to a smaller one. Applications include NLP and object detection allowing for less powerful hardware to make good inferences without significant loss of accuracy."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q238",
        "question": "What is an inductive bias?",
        "answers": [
            "A model's inductive bias is referred to as assumptions made within that model to learn your target function from independent variables, your features. Without these assumptions, there is a whole space of solutions to our problem and finding the one that works best becomes a problem. Found this StackOverflow question useful to look at and explore."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q239",
        "question": "What is the curse of dimensionality?",
        "answers": [
            "1 High dimensionality makes clustering hard, because having lots of dimensions means that everything is far away from each other. 2 For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases 3 All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample. 4 The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data.  5 We should conduct PCA to reduce dimensionality"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q240",
        "question": "Is more data always better?",
        "answers": [
            "1. Statistically: It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help. It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc. 2. Practically, Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q241",
        "question": "What are advantages of plotting your data before per- forming analysis?",
        "answers": [
            "Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman. Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful. Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect. "
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q242",
        "question": "How can you make sure that you don’t analyze something that ends up meaningless?",
        "answers": [
            "Proper exploratory data analysis. In every data analysis task, there's the exploratory phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further. Then there's the exploitatory phase, where you look deeply into a set of hypotheses.  The exploratory phase will generate lots of possible hypotheses, and the exploitatory phase will let you really understand a few of them. Balance the two and you'll prevent yourself from wasting time on many things that end up meaningless, although not all."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q243",
        "question": "What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?",
        "answers": [
            "data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis. The scientific method is eminently inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q244",
        "question": "How can you determine which features are the most im- portant in your model?",
        "answers": [
            "run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles. Look at the variables added in forward variable selection "
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q245",
        "question": "How do you deal with some of your predictors being missing?",
        "answers": [
            "Remove rows with missing values - This works well if 1) the values are missing randomly (see Vinay Prabhu's answer for more details on this) 2) if you don't lose too much of the dataset after doing so. Build another predictive model to predict the missing values - This could be a whole project in itself, so simple techniques are usually used here. Use a model that can incorporate missing data - Like a random forest, or any tree-based method."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q246",
        "question": "You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?",
        "answers": [
            "Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related.  Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based. principal component regression"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q247",
        "question": "Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?",
        "answers": [
            "PCA"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q248",
        "question": "Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?",
        "answers": [
            "1 ridge / lasso / elastic net regression 2 Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores 3 Recursive Feature Elimination: First, train a model with all the feature and evaluate its performance on held out data. Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining features. Iterate until you observe a sharp drop in the predictive accuracy of the model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q249",
        "question": "Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?",
        "answers": [
            "p > n. If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. "
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q250",
        "question": "You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?",
        "answers": [
            "The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q251",
        "question": "What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?",
        "answers": [
            "1 The assumption is that a group of weak learners can be combined to form a strong learner. 2 Hence the combined model is expected to perform better than an individual model. 3 Assumptions: average out biases. reduce variance 4 Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability leads to lower error. 5 Boosting works because of the focus on better defining the decision edge. 6 "
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q252",
        "question": "Given that you have wi  data in your o ce, how would you determine which rooms and areas are underutilized and overutilized?",
        "answers": [
            "If the data is more used in one room, then that one is over utilized! Maybe account for the room capacity and normalize the data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q253",
        "question": "How would you quantify the influence of a Twitter user?",
        "answers": [
            "like page rank with each user corresponding to the webpages and linking to the page equivalent to following."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q254",
        "question": "You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of di culty?",
        "answers": [
            "One way you could do this is by storing a skill level for each user and a difficulty level for each problem.  We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q255",
        "question": "You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?",
        "answers": [
"Some people would take the mean rank of each sushi.  If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q256",
        "question": "Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?",
        "answers": [
            "collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative. For liberal and republican parties, find the mean vector and find the representative closest to the center point"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q257",
        "question": "How would you come up with an algorithm to detect plagiarism in online content?",
        "answers": [
"reduce the text to a more compact form (e.g. fingerprinting, bag of words) then compare those with other texts by calculating the similarity"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q258",
        "question": "Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 offspring, respectively. Each of Bobo’s descendants also have the same probabilities. What is the probability that Bobo’s lineage dies out?",
        "answers": [
"p=1/4+1/4p+1/2p^2 => p=1/2"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q259",
        "question": "In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba- bility that you see at least one shooting star in the period of an hour?",
        "answers": [
"1-(0.8)^4. Or, we can use Poisson processes"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q260",
        "question": "How can you generate a random number between 1 - 7 with only a die?",
        "answers": [
"Launch it 3 times: each throw sets the nth bit of the result. For each launch, if the value is 1-3, record a 0, else 1. The result is between 0 (000) and 7 (111), evenly spread (3 independent throw). Repeat the throws if 0 was obtained: the process stops on evenly spread values."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q261",
        "question": "How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?",
        "answers": [
"Flip twice and if HT then H, TH then T."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q262",
        "question": "You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?",
        "answers": [
            "more than two standard deviations"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q263",
        "question": "Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?",
        "answers": [
            "plug in the value to the CDF of the same random variable"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q264",
        "question": "A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?",
        "answers": [
            "1/3"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q265",
        "question": "You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?",
        "answers": [
            "Gender ratio is 1:1. Expected number of children is 2. let X be the number of children until getting a female (happens with prob 1/2). this follows a geometric distribution with probability 1/2"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q266",
        "question": "How many ways can you split 12 people into 3 teams of 4?",
        "answers": [
            "The outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q267",
        "question": "Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.",
        "answers": [
"the probability of a hash collision: 1-(10!/10^10), the expected number of hash collisions: 1-10*(9/10)^10, the expected number of hashes that are unused: 10*(9/10)^10"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q268",
        "question": "You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?",
        "answers": [
"All Lyft's first: probability that the first car is Lyft = 3/5, probability that the second car is Lyft = 2/4, probability that the third car is Lyft = 1/3 Therefore, probability that all the Lyfts arrive first = (3/5) * (2/4) * (1/3) = 1/10. All Uber's first: probability that the first car is Uber = 2/5, probability that the second car is Uber = 1/4 Therefore, probability that all the Ubers arrive first = (2/5) * (1/4) = 1/10"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q269",
        "question": "I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?",
        "answers": [
            "100+60-20=140"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q270",
        "question": "On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?",
        "answers": [
            "24C5*(1+5(24-5))/24C5*24C5 = 4/1771"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q271",
        "question": "A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?",
        "answers": [
            "1"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q272",
        "question": "Let’s say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?",
        "answers": [
            "shorter. Regression to the mean"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q273",
        "question": "What’s the expected number of coin flips until you get two heads in a row? What’s the expected number of coin flips until you get two tails in a row?",
        "answers": [
"After the first two flips, you can see this problem as a Markov chain, with states HH, HT, TH, TT. HH is the final state. You can than define the expected number of steps N before reaching HH: E(N) = 2 + 0.25nHH, 0.25nHT, 0.25nTH, 0.25nTT. nXX represents the expected number of steps before reaching HH starting from state XX. Solve linear equation: nHH = 0, nHT = 1 + 0.5nTT + 0.5nTH, nTH = 1 + 0.5nHH + 0.5nHT, nTT = 1 + 0.5nTH + 0.5nTT. Result gives E(N) = 6."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q274",
        "question": "Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?",
        "answers": [
            "less than $3"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q275",
        "question": "You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?",
        "answers": [
            "4/13"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q276",
        "question": "You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What’s the chance that you picked up the fair coin, given the information that you observed?",
        "answers": [
"Events: F = picked a fair coin, T = 10 heads in a row. (1) P(F|T) = P(T|F)P(F)/P(T) (Bayes formula). (2) P(T) = P(T|F)P(F) + P(T|¬F)P(¬F) (total probabilities formula). Injecting (2) in (1): P(F|T) = P(T|F)P(F)/(P(T|F)P(F) + P(T|¬F)P(¬F)) = 1 / (1 + P(T|¬F)P(¬F)/(P(T|F)P(F))). Numerically: 1/(1 + 0.001 * 2^10 /0.999).With 2^10 ≈ 1000 and 0.999 ≈ 1 this simplifies to 1/2"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q277",
        "question": "What is a P-Value ?",
        "answers": [
"The probability to obtain a similar or more extreme result than observed when the null hypothesis is assumed. If the p-value is small, the null hypothesis is unlikely"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q278",
        "question": "In an A/B test, how can you check if assignment to the various buckets was truly random?",
        "answers": [
            "Plot the distributions of multiple features for both A and B and make sure that they have the same shape. More rigorously, we can conduct a permutation test to see if the distributions are the same. MANOVA to compare different means"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q279",
        "question": "What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?",
        "answers": [
            "verify the sampling algorithm is random."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q280",
        "question": "What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?",
        "answers": [
"The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q281",
        "question": "What would be some issues if blogs decide to cover one of your experimental groups?",
        "answers": [
            "Same as the previous question. The above problem can happen in larger scale."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q282",
        "question": "How would you conduct an A/B test on an opt-in feature? ",
        "answers": [
            "Ask someone for more details."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q283",
        "question": "How would you run an A/B test for many variants, say 20 or more?",
        "answers": [
            "one control, 20 treatment, if the sample size for each group is big enough. Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q284",
        "question": "How would you run an A/B test if the observations are extremely right-skewed?",
        "answers": ["lower the variability by modifying the KPI, cap values, percentile metrics, log transform"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q285",
        "question": "I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?",
        "answers": [
            "Exclusive -> ok"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q286",
        "question": "What is a p-value? What is the difference between type-1 and type-2 error?",
        "answers": [
            "A p-value is defined such that under the null hypothesis less than the fraction p of events have parameter values more extreme than the observed parameter. It is not the probability that the null hypothesis is wrong. type-1 error: rejecting Ho when Ho is true type-2 error: not rejecting Ho when Ha is true"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q287",
        "question": "You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?",
        "answers": [
            "For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups. Ask someone for more details."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q288",
        "question": "How would you design an experiment to determine the impact of latency on user engagement?",
        "answers": [
            "The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, i.e., add a delay in an A/B test."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q289",
        "question": "What is maximum likelihood estimation? Could there be any case where it doesn’t exist?",
        "answers": [
"A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model). maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized. for gaussian mixtures, non parametric models, it doesn’t exist"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q290",
        "question": "What’s the difference between a MAP, MOM, MLE estima- tor? In which cases would you want to use each?",
        "answers": [
            "MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution. MOM sets moment values and solves for the parameters. MOM is not used much anymore because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q291",
        "question": "What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?",
        "answers": [
            "Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However, this is not always desirable for data analysis or predictive modeling as there is the bias variance tradeoff. We sometimes want to prioritize the generalizability and avoid overfitting by reducing variance and thus increasing bias."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q292",
        "question": "What is Selection Bias?",
        "answers": [
            "Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn’t random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q293",
        "question": "You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)",
        "answers": [
            "Processing a high dimensional data on a limited memory machine is a strenuous task, your interviewer would be fully aware of that."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q294",
        "question": "Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the components?",
        "answers": [
            "Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, that’s the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesn’t change, it only changes the actual coordinates of the points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q295",
        "question": "You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?",
        "answers": [
            "This question has enough hints for you to start thinking! Since, the data is spread across median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q296",
        "question": "You are given a data set on cancer detection. You’ve build a classification model and achieved an accuracy of 96%. Why shouldn’t you be happy with your model performance? What can you do about it?",
        "answers": [
            "If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q297",
        "question": "Why is naive Bayes so ‘naive’ ?",
        "answers": [
            "naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are equally important and independent. As we know, these assumption are rarely true in real world scenario."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q298",
        "question": "Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?",
        "answers": [
            "Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new email would be classified as spam."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q299 ",
        "question": "You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?",
        "answers": [
            "Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q300",
        "question": "You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?",
        "answers": [
            "You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q301",
        "question": "You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?",
        "answers": [
            "Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q302",
        "question": "You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?",
        "answers": [
            "Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q303",
        "question": "After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?",
        "answers": [
            "As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior results when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q304",
        "question": "How is kNN different from kmeans clustering?",
        "answers": [
            "Don’t get mislead by ‘k’ in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q305",
        "question": "How is True Positive Rate and Recall related? Write the equation.",
        "answers": [
            "True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q306",
        "question": "You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?",
        "answers": [
            "Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is predicted value.  "
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q307",
        "question": "After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?",
        "answers": [
            "To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity. But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q308",
        "question": "When is Ridge regression favorable over Lasso regression?",
        "answers": [
            "You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q309",
        "question": "Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?",
        "answers": [
            "After reading this question, you should have understood that this is a classic case of “causation and correlation”. No, we can’t conclude that decrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q310",
        "question": "While working on a data set, how do you select important variables? Explain your methods.",
        "answers": [
            "Following are the methods of variable selection you can use: 1 Remove the correlated variables prior to selecting important variables. 2 Use linear regression and select variables based on p values. 3 Use Forward Selection, Backward Selection, Stepwise Selection. 4 Use Random Forest, Xgboost and plot variable importance chart. 5 Use Lasso Regression. 6 Measure information gain for the available set of features and select top n features accordingly."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q311",
        "question": "What is the difference between covariance and correlation?",
        "answers": [
            "orrelation is the standardized form of covariance. Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we’ll get different covariances which can’t be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q312",
        "question": "Is it possible capture the correlation between continuous and categorical variable? If yes, how?",
        "answers": [
            "Answer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q313",
        "question": "Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?",
        "answers": [
            "The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q314",
        "question": "Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?",
        "answers": [
            "A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data set into purest possible children nodes."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q315",
        "question": "You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?",
        "answers": [
            "The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldn’t find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q316",
        "question": "You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?",
        "answers": [
            "In such high dimensional data sets, we can’t use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q317",
        "question": "What is convex hull ? (Hint: Think SVM)",
        "answers": [
            "In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q318",
        "question": "We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?",
        "answers": [
            "Don’t get baffled at this question. It’s a simple question asking the difference between the two. Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: let’s say we have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot encoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value. In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q319",
        "question": "What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?",
        "answers": [
            "Neither. In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we might end up validation on past years, which is incorrect."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q320",
        "question": "You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?",
        "answers": [
            "We can deal with them in the following ways: 1 Assign a unique category to missing values, who knows the missing values might decipher some tren. 2 We can remove them blatantly. 3 Or, we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q321",
        "question": "People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?",
        "answers": [
            "The basic idea for this kind of recommendation engine comes from collaborative filtering. Collaborative Filtering algorithm considers “User Behavior” for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q322",
        "question": "What do you understand by Type I vs Type II error ?",
        "answers": [
            "Type I error is committed when the null hypothesis is true and we reject it, also known as a ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q323",
        "question": "You are working on a classification problem. For validation purposes, you’ve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?",
        "answers": [
            "In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesn’t takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q324",
        "question": "You have been asked to evaluate a regression model based on R², adjusted R² and tolerance. What will be your criteria?",
        "answers": [
            "Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q325",
        "question": "In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?",
        "answers": [
            "We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q326",
        "question": "Explain machine learning to me like a 5 year old.",
        "answers": [
            "It’s simple. It’s just like how babies learn to walk. Every time they fall down, they learn (unconsciously) & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that again’. In order to avoid that pain, they try harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q327",
        "question": "I know that a linear regression model is generally evaluated using Adjusted R² or F value. How would you evaluate a logistic regression model?",
        "answers": [
            "We can use the following methods: 1 Since logistic regression is used to predict probabilities, we can use AUC-ROC curve along with confusion matrix to determine its performance. 2 Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value. 3 Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q328",
        "question": "Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?",
        "answers": [
            "You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q329",
        "question": "Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?",
        "answers": [
            "For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q330",
        "question": "When does regularization becomes necessary in Machine Learning?",
        "answers": [
            "Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q331",
        "question": "What do you understand by Bias Variance trade off?",
        "answers": [
            "The error emerging from any model can be broken down into three components mathematically."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q332",
        "question": "OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.",
        "answers": [
            "OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q333",
        "question": "What is Date Science?",
        "answers": [
            "An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q334",
        "question": "What are lambda functions in Python?",
        "answers": [
            "Lambda functions in Python are anonymous functions created using the lambda keyword. They are useful when you need a small, simple function for a short period and don't want to formally define a full function using the def keyword. Lambda functions are often used for quick operations where a full function definition seems unnecessary."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q335",
        "question": "Explain the differences between lists and tuples in Python",
        "answers": [
            "In Python, both lists and tuples are used to store collections of items, but there are some key differences between them."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q336",
        "question": "What is the difference between lists, arrays, and sets in Python, and when you should use each of them?",
        "answers": [
            "In Python, lists, arrays, and sets are different data structures, each with its own characteristics and use cases."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q337",
        "question": "Explain the Global Interpreter Lock (GIL) in Python and its impact on multi-threaded programs.",
        "answers": [
            "The Global Interpreter Lock (GIL) in Python is a mechanism that ensures only one thread executes Python bytecodes at a time in a single process. It is a mutex (mutual exclusion) that protects access to Python objects, preventing multiple native threads from executing Python bytecodes simultaneously. The GIL is necessary because CPython, the reference implementation of Python, is not thread-safe when it comes to memory management."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q338",
        "question": "What is the purpose of virtual environments in Python, and how do you create one?",
        "answers": [
            "Virtual environments in Python are a way to create isolated environments for Python projects. Each virtual environment has its own Python binary and set of installed packages, allowing you to manage dependencies and avoid conflicts between different projects. This is particularly useful when working on multiple projects that may have different requirements or dependencies."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q339",
        "question": "How does Python’s garbage collection work?",
        "answers": [
            "Python’s garbage collection is an automatic memory management system that helps reclaim memory occupied by objects that are no longer in use. The primary mechanism for garbage collection in Python is a combination of reference counting and a cyclic garbage collector."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q340",
        "question": "Explain the differences between shallow copy and deep copy in Python.",
        "answers": [
            "In Python, the concepts of shallow copy and deep copy are related to duplicating objects, especially when dealing with nested data structures like lists or dictionaries."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q341",
        "question": "What is the purpose of the zip() function in Python? Provide an example.",
        "answers": [
            "The zip() function in Python is used to combine elements from multiple iterable objects (such as lists or tuples) into tuples. It aggregates the items at the same index from each iterable and creates an iterator that produces tuples containing elements at the same positions. If the input iterables are of different lengths, zip() stops creating tuples when the shortest input iterable is exhausted."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q342",
        "question": "Explain the use of regular expressions in Python. Provide an example.",
        "answers": [
            "Regular expressions (often abbreviated as regex or regexp) are a powerful tool for pattern matching and string manipulation in Python. The re module in Python provides support for regular expressions. Regular expressions allow you to search, match, and manipulate strings based on a specified pattern."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q343",
        "question": "What is the purpose of the __init__ method in Python classes?",
        "answers": [
            "In Python, the __init__ method is a special method, also known as the constructor, that is automatically called when an object of a class is created. Its primary purpose is to initialize the attributes of the object"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q344",
        "question": "What is the purpose of the if __name__ == __main__: statement in Python scripts?",
        "answers": [
            "The if __name__ == __main__: statement in Python scripts serves a specific purpose related to the execution of the script. It provides a way to determine whether the Python script is being run as the main program or if it is being imported as a module into another script."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q345",
        "question": "Explain the use of the map() function in Python. Provide an example.",
        "answers": [
            "The map() function in Python is a built-in function that applies a specified function to all items in an iterable (e.g., a list) and returns an iterator that produces the results. The map() function is often used to transform data by applying a given function to each element of an iterable."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q346",
        "question": "What is the purpose of NumPy in Python?",
        "answers": [
            "NumPy is a powerful numerical computing library in Python that serves several key purposes: 1 Efficient Array Operations: Provides the ndarray object for efficient handling of large, multi-dimensional arrays. 2 Mathematical Functions: Offers a variety of mathematical functions for efficient element-wise operations on arrays. 3 Broadcasting: Supports broadcasting, allowing operations between arrays of different shapes without explicit loops. 4 Linear Algebra Operations: Includes a comprehensive set of linear algebra functions for matrix operations. 5 Random Number Generation: Provides functions for generating random numbers from various distributions. 6 Integration with Other Libraries: Foundational for many scientific computing libraries, enhancing interoperability. 7 Memory Efficiency: Offers memory-efficient arrays, especially beneficial for large datasets. 8 NumPy is widely used in scientific computing, machine learning, and data analysis, making it a fundamental part of the Python scientific computing ecosystem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q347",
        "question": "Explain the concept of broadcasting in NumPy",
        "answers": [
            "Broadcasting in NumPy is a powerful feature that allows for element-wise operations between arrays of different shapes and sizes. It provides a way to perform operations on arrays without explicitly reshaping them to the same shape, making the code more concise and readable."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q348",
        "question": "What is the difference between loc and iloc in Pandas?",
        "answers": [
            "In Pandas, loc and iloc are two methods used for indexing and selecting data from a DataFrame. They have some key differences in terms of how they interpret the indices and positions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }, 
{
        "id": "Q349",
        "question": "What is the difference between apply and applymap functions in pandas?",
        "answers": [
            "In Pandas, both apply and applymap are functions used for applying a function to elements of a DataFrame."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }

]