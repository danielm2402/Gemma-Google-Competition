[
    {
        "id": "Q1",
        "question": "Explain the central limit theorem and give examples of when you can use it in a real-world problem.",
        "answer": [
            "The center limit theorem states that if any random variable, regardless of the distribution, is sampled a large enough time, the sample mean will be approximately normally distributed. This allows for studying the properties of any statistical distribution as long as there is a large enough sample size.",
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q2",
        "question": "Briefly explain the A/B testing and its application? What are some common pitfalls encountered in A/B testing?",
        "answer": [
            "/B testing helps us to determine whether a change in something will cause a change in performance significantly or not. So in other words you aim to statistically estimate the impact of a given change within your digital product (for example). You measure success and counter metrics on at least 1 treatment vs 1 control group (there can be more than 1 XP group for multivariate tests)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q3",
        "question": "Describe briefly the hypothesis testing and p-value in layman’s term? And give a practical application for them ?",
        "answer": [
            "Hypothesis test is where you have a current state (null hypothesis) and an alternative state (alternative hypothesis). You assess the results of both of the states and see some differences. You want to decide whether the difference is due to the alternative approach or not."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q4",
        "question": "Mention three ways to make your model robust to outliers.",
        "answer": [
            "Investigating the outliers is always the first step in understanding how to treat them. After you understand the nature of why the outliers occurred you can apply one of the several methods mentioned"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q5",
        "question": "What is the meaning of selection bias and how to avoid it?",
        "answer": [
            "Sampling bias is the phenomenon that occurs when a research study design fails to collect a representative sample of a target population. This typically occurs because the selection criteria for respondents failed to capture a wide enough sampling frame to represent all viewpoints."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q6",
        "question": "Explain the long-tailed distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?",
        "answer": [
            "A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q7",
        "question": "What is the meaning of KPI in statistics",
        "answer": [
            "KPI stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. From finance and HR to marketing and sales, key performance indicators help every area of the business move forward at the strategic level. Types of KPIs Key performance indicators come in many flavors. While some are used to measure monthly progress against a goal, others have a longer-term focus. The one thing all KPIs have in common is that they’re tied to strategic goals. Here’s an overview of some of the most common types of KPIs."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q8",
        "question": "Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not?",
        "answer": [
            "The null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased. The p-value is the probability of observing the results obtained given that the null hypothesis is true, in this case, the coin is fair."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q9",
        "question": "Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not?",
        "answer": [
            "The main consideration when we have a large number of tests is that probability of getting a significant test due to chance alone increases. This will increase the type 1 error (rejecting the null hypothesis when it's actually true. Therefore we need to consider the Bonferroni Effect which happens when we make many tests. Ex. If our significance level is 0.05 but we made a 100 test it means that the probability of getting a value inside the rejection rejoin is 0.0005, not 0.05 so here we need to use another significance level which's called alpha star = significance level /K Where K is the number of the tests"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q10",
        "question": "What general conditions must be satisfied for the central limit theorem to hold?",
        "answer": [
            "In order to apply the central limit theorem, there are four conditions that must be met: 1.** Randomization:** The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample. Independence: The sample values must be independent of each other. The 10% Condition: When the sample is drawn without replacement, the sample size should be no larger than 10% of the population. Large Sample Condition: The sample size needs to be sufficiently large."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q11",
        "question": "What is skewness discuss two methods to measure it?",
        "answer": [
            "Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed.Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. There are two main types of skewness negative skew which refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. These two skews refer to the direction or weight of the distribution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q12",
        "question": "You sample from a uniform distribution [0, d] n times. What is your best estimate of d?",
        "answer": [
            "Intuitively it is the maximum of the sample points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q13",
        "question": "Discuss the Chi-square, ANOVA, and t-test",
        "answer": [
            "Chi-square test A statistical method is used to find the difference or correlation between the observed and expected categorical variables in the dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q14",
        "question": "What is the relationship between the significance level and the confidence level in Statistics?",
        "answer": [
            "Confidence level = 1 - significance level. It's closely related to hypothesis testing and confidence intervals. Significance Level according to the hypothesis testing literature means the probability of Type-I error one is willing to tolerate. Confidence Level according to the confidence interval literature means the probability in terms of the true parameter value lying inside the confidence interval. They are usually written in percentages."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q15",
        "question": "What is the Law of Large Numbers in statistics and how it can be used in data science ?",
        "answer": [
            "The law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates. In data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modeling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data. The law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q16",
        "question": "What is the difference between a confidence interval and a prediction interval, and how do you calculate them?",
        "answer": [
            "A confidence interval is a range of values that is likely to contain the true value of a population parameter with a certain level of confidence. It is used to estimate the precision or accuracy of a sample statistic, such as a mean or a proportion, based on a sample from a larger population."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q17",
        "question": "You and your friend are playing a game with a fair coin. The two of you will continue to toss the coin until the sequence HH or TH shows up. If HH shows up first, you win, and if TH shows up first your friend win. What is the probability of you winning the game?",
        "answer": [
            "If T is ever flipped, you cannot then reach HH before your friend reaches TH. Therefore, the probability of you winning this is to flip HH initially. Therefore the sample space will be {HH, HT, TH, TT} and the probability of you winning will be (1/4) and your friend (3/4)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q18",
        "question": "If you roll a dice three times, what is the probability to get two consecutive threes?If you roll a dice three times, what is the probability to get two consecutive threes?",
        "answer": [
            "The right answer is 11/216."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q19",
        "question": "Suppose you have ten fair dice. If you randomly throw them simultaneously, what is the probability that the sum of all of the top faces is divisible by six?",
        "answer": [
            "1/6. Explanation: With 10 dices, the possible sums divisible by 6 are 12, 18, 24, 30, 36, 42, 48, 54, and 60. You don't actually need to calculate the probability of getting each of these numbers as the final sums from 10 dices because no matter what the sum of the first 9 numbers is, you can still choose a number between 1 to 6 on the last die and add to that previous sum to make the final sum divisible by 6. Therefore, we only care about the last die. And the probability to get that number on the last die is 1/6. So the answer is 1/6"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q20",
        "question": "If you have three draws from a uniformly distributed random variable between 0 and 2, what is the probability that the median of three numbers is greater than 1.5?",
        "answer": [
            "The right answer is 5/32 or 0.156. There are different methods to solve it: Method 1: To get a median greater than 1.5 at least two of the three numbers must be greater than 1.5. The probability of one number being greater than 1.5 in this distribution is 0.25. Then, using the binomial distribution with three trials and a success probability of 0.25 we compute the probability of 2 or more successes to get the probability of the median is more than 1.5, which would be about 15.6%. Method2 : A median greater than 1.5 will occur when o all three uniformly distributed random numbers are greater than 1.5 or 1 uniform distributed random number between 0 and 1.5 and the other two are greater than 1.5. So, the probability of the above event is = {(2 - 1.5) / 2}^3 + (3 choose 1)(1.5/2)(0.5/2)^2 = 10/64 = 5/32"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q21",
        "question": "Assume you have a deck of 100 cards with values ranging from 1 to 100 and you draw two cards randomly without replacement, what is the probability that the number of one of them is double the other?",
        "answer": [
            "There are a total of (100 C 2) = 4950 ways to choose two cards at random from the 100 cards and there are only 50 pairs of these 4950 ways that you will get one number and it's double. Therefore the probability that the number of one of them is double the other is 50/4950."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q22",
        "question": "If there are 30 people in a room, what is the probability that everyone has different birthdays?",
        "answer": [
            "The sample space is 365^30 and the number of events is 365p30 because we need to choose persons without replacement to get everyone to have a unique birthday therefore the Prob = 356p30 / 365^30 = 0.2936"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q23",
        "question": "Assume two coins, one fair and the other is unfair. You pick one at random, flip it five times, and observe that it comes up as tails all five times. What is the probability that you are fliping the unfair coin?",
        "answer": [
            "Let's use Baye’s theorem let U denote the case where you are flipping the unfair coin and F denote the case where you are flipping the fair coin. Since the coin is chosen randomly, we know that P(U)=P(F)=0.5. Let 5T denote the event of flipping 5 tails in a row. Then, we are interested in solving for P(U|5T) (the probability that you are flipping the unfair coin given that you obtained 5 tails). Since the unfair coin always results in tails, therefore P(5T|U) = 1 and also P(5T|F) =1/2⁵ = 1/32 by the definition of a fair coin. Lets apply Bayes theorem where P(U|5T) = P(5T|U) * P(U) / P(5T|U)* P(U) + P(5T|F)* P(F) = 0.5 / 0.5 +0.5* 1/32 = 0.97. Therefore the probability that you picked the unfair coin is 97%"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q24",
        "question": "Assume you take a stick of length 1 and you break it uniformly at random into three parts. What is the probability that the three pieces can be used to form a triangle?",
        "answer": [
            "The right answer is 0.25. Let's say, x and y are the lengths of the two parts, so the length of the third part will be 1-x-y. As per the triangle inequality theorem, the sum of two sides should always be greater than the third side. Therefore, no two lengths can be more than 1/2. x<1/2 y<1/2. Based on the triangle inequality theorem: x+y > 1-a-b x+y > 1/2. From the diagram below, there is only one triangle that matches all the above conditions out of 4 triangles. Therefore, the probability will be 1/4"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q25",
        "question": "Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?",
        "answer": [
            "For making 2 chords, 4 points are necessary and from 4 points there are 3 different combinations of pairs of chords can be made. From the 3 combinations, there is only one combination in which the two chords intersect hence answer is 1/3. Let's assume that P1, P2, P3, and P4 are four points then 3 different combinations are possible for pairs of chords: (P1 P2) (P3 P4) or (P1 P3) (P4 P2) or (P1 P4) (P2 P3) there the 3rd one will only intersect."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q26",
        "question": "If there’s a 15% probability that you might see at least one airplane in a five-minute interval, what is the probability that you might see at least one airplane in a period of half an hour?",
        "answer": [
            "Probability of at least one plane in 5 mins interval=0.15 Probability of no plane in 5 mins interval=0.85 Probability of seeing at least one plane in 30 mins=1 - Probability of not seeing any plane in 30 minutes =1-(0.85)^6 = 0.6228"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q27",
        "question": "According to hospital records, 75% of patients suffering from a disease die from that disease. Find out the probability that 4 out of the 6 randomly selected patients survive.",
        "answer": [
            "This has to be a binomial since there are only 2 outcomes – death or life"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q28",
        "question": "You have 40 cards in four colors, 10 reds, 10 greens, 10 blues, and ten yellows. Each color has a number from 1 to 10. When you pick two cards without replacement, what is the probability that the two cards are not in the same color and not in the same number?",
        "answer": [
            "Since it doesn't matter how you choose the first card, so, choose one card at random. Now, all we have to care about is the restriction on the second card. It can't be the same number (i.e. 3 cards from the other colors can't be chosen in favorable cases) and also can't be the same color (i.e. 9 cards from the same color can't be chosen keep in mind we have already picked one). So, the number of favorable choices for the 2nd card is (39-12)/39 = 27/39 = 9/13"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q29",
        "question": "Can you explain the difference between frequentist and Bayesian probability approaches?",
        "answer": [
            "The frequentist approach to probability defines probability as the long-run relative frequency of an event in an infinite number of trials. It views probabilities as fixed and objective, determined by the data at hand. In this approach, the parameters of a model are treated as fixed and unknown and estimated using methods like maximum likelihood estimation."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q30",
        "question": "Explain the Difference Between Probability and Likelihood",
        "answer": [
            "Probability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses. Probability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5. The likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q31",
        "question": "Describe the motivation behind random forests and mention two reasons why they are better than individual decision trees.",
        "answer": [
            "The motivation behind random forest or ensemble models in general in layman's terms, Let's say we have a question/problem to solve we bring 100 people and ask each of them the question/problem and record their solution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q32",
        "question": "What are the differences and similarities between gradient boosting and random forest? and what are the advantages and disadvantages of each when compared to each other?",
        "answer": [
            "Similarities: (a) Both these algorithms are decision-tree-based algorithms. (b) Both these algorithms are ensemble algorithms. (c) Both are flexible models and do not need much data preprocessing."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q33",
        "question": "What are L1 and L2 regularization? What are the differences between the two?",
        "answer": [
            "Regularization is a technique used to avoid overfitting by trying to make the model more simple."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q34",
        "question": "What are the Bias and Variance in a Machine Learning Model and explain the bias-variance trade-off?",
        "answer": [
            "The goal of any supervised machine learning model is to estimate the mapping function (f) that predicts the target variable (y) given input (x)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q35",
        "question": "Mention three ways to handle missing or corrupted data in a dataset",
        "answer": [
            "In general, real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q36",
        "question": "Explain briefly the logistic regression model and state an example of when you have used it recently",
        "answer": [
            "Logistic regression is used to calculate the probability of occurrence of an event in the form of a dependent output variable based on independent input variables. Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. If the probability is bigger than 0.5 then it will belong to that class (positive) and if it is below 0.5 it will belong to the other class. This will make it a binary classifier."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q37",
        "question": "Explain briefly batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. and what are the pros and cons for each of them?",
        "answer": [
            "Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function. Batch Gradient Descent: In Batch Gradient descent the whole training data is used to minimize the loss function by taking a step toward the nearest minimum by calculating the gradient (the direction of descent). Pros: Since the whole data set is used to calculate the gradient it will be stable and reach the minimum of the cost function without bouncing (if the learning rate is chosen cooreclty)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q38",
        "question": "Explain what is information gain and entropy in the context of decision trees.",
        "answer": [
            "Entropy and Information Gain are two key metrics used in determining the relevance of decision-making when constructing a decision tree model and determining the nodes and the best way to split. The idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label. Entropy is the measure of impurity, disorder, or uncertainty in a bunch of examples. Entropy controls how a Decision Tree decides to split the data. Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q39",
        "question": "Explain the linear regression model and discuss its assumption.",
        "answer": [
            "Linear regression is a supervised statistical model to predict dependent variable quantity based on independent variables. Linear regression is a parametric model and the objective of linear regression is that it has to learn coefficients using the training data and predict the target value given only independent values."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q40",
        "question": "Explain briefly the K-Means clustering and how can we find the best value of K?",
        "answer": [
            "K-Means is a well-known clustering algorithm. K-means clustering is often used because it is easy to interpret and implement."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q41",
        "question": "Define Precision, recall, and F1 and discuss the trade-off between them?",
        "answer": [
            "Precision and recall are two classification evaluation metrics that are used beyond accuracy."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q42",
        "question": "What are the differences between a model that minimizes squared error and the one that minimizes the absolute error? and in which cases each error metric would be more appropriate?",
        "answer": [
            "Both mean square error (MSE) and mean absolute error (MAE) measures the distances between vectors and express average model prediction in units of the target variable. Both can range from 0 to infinity, the lower they are the better the model. The main difference between them is that in MSE the errors are squared before being averaged while in MAE they are not. This means that a large weight will be given to large errors. MSE is useful when large errors in the model are trying to be avoided. This means that outliers affect MSE more than MAE, that is why MAE is more robust to outliers. Computation-wise MSE is easier to use as the gradient calculation will be more straightforward than MAE, which requires linear programming to calculate it."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q43",
        "question": "Define and compare parametric and non-parametric models and give two examples for each of them?",
        "answer": [
            "Parametric models assume that the dataset comes from a certain function with some set of parameters that should be tuned to reach the optimal performance. For such models, the number of parameters is determined prior to training, thus the degree of freedom is limited and reduces the chances of overfitting."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q44",
        "question": "Explain the kernel trick in SVM. Why do we use it and how to choose what kernel to use?",
        "answer": [
            "Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q45",
        "question": "Define the cross-validation process and the motivation behind using it",
        "answer": [
            "Cross-validation is a technique used to assess the performance of a learning model in several subsamples of training data. In general, we split the data into train and test sets where we use the training data to train our model and the test data to evaluate the performance of the model on unseen data and validation set for choosing the best hyperparameters. Now, a random split in most cases(for large datasets) is fine. However, for smaller datasets, it is susceptible to loss of important information present in the data in which it was not trained. Hence, cross-validation though computationally expensive combats this issue."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q46",
        "question": "You are building a binary classifier and you found that the data is imbalanced, what should you do to handle this situation?",
        "answer": [
            "If there is a data imbalance there are several measures we can take to train a fairer binary classifier: (a) Pre-Processing: Check whether you can get more data or not. Use sampling techniques (Sample minority class, Downsample majority class, can take the hybrid approach as well). We can also use data augmentation to add more data points for the minority class but with little deviations/changes leading to new data points that are similar to the ones they are derived from. The most common/popular technique is SMOTE (Synthetic Minority Oversampling technique). Suppression: Though not recommended, we can drop off some features directly responsible for the imbalance. Learning Fair Representation: Projecting the training examples to a subspace or plane minimizes the data imbalance. Re-Weighting: We can assign some weights to each training example to reduce the imbalance in the data. (b) In-Processing: Regularisation: We can add score terms that measure the data imbalance in the loss function and therefore minimizing the loss function will also minimize the degree of imbalance concerning the score chosen which also indirectly minimizes other metrics that measure the degree of data imbalance. Adversarial Debiasing: Here we use the adversarial notion to train the model where the discriminator tries to detect if there are signs of data imbalance in the predicted data by the generator and hence the generator learns to generate data that is less prone to imbalance. (c) Post-Processing: Odds-Equalization: Here we try to equalize the odds for the classes with respect to the data is imbalanced for correct imbalance in the trained model. Usually, the F1 score is a good choice, if both precision and recall scores are important. Choose appropriate performance metrics. For example, accuracy is not a correct metric to use when classes are imbalanced. Instead, use precision, recall, F1 score, and ROC curve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q47",
        "question": "You are working on a clustering problem, what are different evaluation metrics that can be used, and how to choose between them?",
        "answer": [
            "Clusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations and similar observations together, then it has performed well. The two most popular metrics evaluation metrics for clustering algorithms are the 𝐒𝐢𝐥𝐡𝐨𝐮𝐞𝐭𝐭𝐞 𝐜𝐨𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭 and 𝐃𝐮𝐧𝐧’𝐬 𝐈𝐧𝐝𝐞𝐱."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q48",
        "question": "What is the ROC curve and when should you use it?",
        "answer": [
            "ROC curve, Receiver Operating Characteristic curve, is a graphical representation of the model's performance where we plot the True Positive Rate (TPR) against the False Positive Rate (FPR) for different threshold values, for hard classification, between 0 to 1 based on model output. This ROC curve is mainly used to compare two or more models as shown in the figure below. Now, it is easy to see that a reasonable model will always give FPR less (since it's an error) than TPR so, the curve hugs the upper left corner of the square box 0 to 1 on the TPR axis and 0 to 1 on the FPR axis."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q49",
        "question": "What is the difference between hard and soft voting classifiers in the context of ensemble learners?",
        "answer": [
            "Hard Voting: We take into account the class predictions for each classifier and then classify an input based on the maximum votes to a particular class. Soft Voting: We take into account the probability predictions for each class by each classifier and then classify an input to the class with maximum probability based on the average probability (averaged over the classifier's probabilities) for that class."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q50",
        "question": "What is boosting in the context of ensemble learners discuss two famous boosting methods",
        "answer": [
            "Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q51",
        "question": "How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?",
        "answer": [
            "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q52",
        "question": "Define the curse of dimensionality and how to solve it.",
        "answer": [
            "Curse of dimensionality represents the situation when the amount of data is too few to be represented in a high-dimensional space, as it will be highly scattered in that high-dimensional space and it becomes more probable that we overfit this data. If we increase the number of features, we are implicitly increasing model complexity and if we increase model complexity we need more data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q53",
        "question": "In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?",
        "answer": [
            "Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q54",
        "question": "Discuss two clustering algorithms that can scale to large datasets",
        "answer": [
            "Minibatch Kmeans: Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)  is a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q55",
        "question": "Do you need to scale your data if you will be using the SVM classifier and discus your answer",
        "answer": [
            "Yes, feature scaling is required for SVM and all margin-based classifiers since the optimal hyperplane (the decision boundary) is dependent on the scale of the input features. In other words, the distance between two observations will differ for scaled and non-scaled cases, leading to different models being generated. This can be seen in the figure below, when the features have different scales, we can see that the decision boundary and the support vectors are only classifying the X1 features without taking into consideration the X0 feature, however after scaling the data to the same scale the decision boundaries and support vectors are looking much better and the model is taking into account both features."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q56",
        "question": "What are Loss Functions and Cost Functions? Explain the key Difference Between them.",
        "answer": [
            "The loss function is the measure of the performance of the model on a single training example, whereas the cost function is the average loss function over all training examples or across the batch in the case of mini-batch gradient descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q57",
        "question": "What is the importance of batch in machine learning and explain some batch-dependent gradient descent algorithms?",
        "answer": [
            "In the memory, the dataset can load either completely at once or in the form of a set. If we have a huge size of the dataset, then loading the whole data into memory will reduce the training speed, hence batch term is introduced."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q58",
        "question": "What are the different methods to split a tree in a decision tree algorithm?",
        "answer": [
            "Decision trees can be of two types regression and classification. For classification, classification accuracy created a lot of instability."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q59",
        "question": "Why boosting is a more stable algorithm as compared to other ensemble algorithms?",
        "answer": [
            "Boosting algorithms focus on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. That’s why boosting is a more stable algorithm compared to other ensemble algorithms."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q60",
        "question": "What is active learning and discuss one strategy of it?",
        "answer": [
            "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes referred to as optimal experimental design."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q61",
        "question": "What are the different approaches to implementing recommendation systems?",
        "answer": [
            "(a) 𝐂𝐨𝐧𝐭𝐞𝐧𝐭-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: Content-Based Filtering depends on similarities of items and users' past activities on the website to recommend any product or service. (b) 𝐂𝐨𝐥𝐥𝐚𝐛𝐨𝐫𝐚𝐭𝐢𝐯𝐞-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: The primary job of a collaborative filtering system is to overcome the shortcomings of content-based filtering. (c) 𝐇𝐲𝐛𝐫𝐢𝐝 𝐟𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: A mixture of content and collaborative methods. Uses descriptors and interactions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q62",
        "question": "What are the evaluation metrics that can be used for multi-label classification?",
        "answer": [
            "Multi-label classification is a type of classification problem where each instance can be assigned to multiple classes or labels simultaneously. The evaluation metrics for multi-label classification are designed to measure the performance of a multi-label classifier in predicting the correct set of labels for each instance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q63",
        "question": "What is the difference between concept and data drift and how to overcome each of them?",
        "answer": [
            "Concept drift and data drift are two different types of problems that can occur in machine learning systems. Concept drift refers to changes in the underlying relationships between the input data and the target variable over time. This means that the distribution of the data that the model was trained on no longer matches the distribution of the data it is being tested on. For example, a spam filter model that was trained on emails from several years ago may not be as effective at identifying spam emails from today because the language and tactics used in spam emails may have changed."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q64",
        "question": "Can you explain the ARIMA model and its components?",
        "answer": [
            "The ARIMA model, which stands for Autoregressive Integrated Moving Average, is a widely used time series forecasting model. It combines three key components: Autoregression (AR), Differencing (I), and Moving Average (MA)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q65",
        "question": "What are the assumptions made by the ARIMA model?",
        "answer": [
            "The ARIMA model makes several assumptions about the underlying time series data. These assumptions are important to ensure the validity and accuracy of the model's results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q66",
        "question": "What is supervised machine learning?",
        "answer": [
            "Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q67",
        "question": "What is regression? Which models can you use to solve a regression problem?",
        "answer": [
            "Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor). Here are some common regression models"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q68",
        "question": "What is linear regression? When do we use it?",
        "answer": [
            "Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q69",
        "question": "What are the main assumptions of linear regression?",
        "answer": [
            "There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q70",
        "question": "What’s the normal distribution? Why do we care about it?",
        "answer": [
            "The normal distribution derives its importance from the Central Limit Theorem, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e the distribution of the mean of the samples is normal. It is important that each sample is independent from the other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q71",
        "question": "How do we check if a variable follows the normal distribution?",
        "answer": [
            "(a) Plot a histogram out of the sampled data. If you can fit the bell-shaped normal curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected. (b) Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution. (c) Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously. (d) Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q72",
        "question": "What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices?",
        "answer": [
            "Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q73",
        "question": "What methods for solving linear regression do you know?",
        "answer": [
            "To solve linear regression, you need to find the coefficients which minimize the sum of squared errors."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q74",
        "question": "What is gradient descent? How does it work?",
        "answer": [
            "Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q75",
        "question": "What is the normal equation?",
        "answer": [
            "Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q76",
        "question": "What is SGD—stochastic gradient descent? What’s the difference with the usual gradient descent?",
        "answer": [
            "In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function. The difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q77",
        "question": "Which metrics for evaluating regression models do you know?",
        "answer": [
            "(1) Mean Squared Error(MSE) (2) Root Mean Squared Error(RMSE) (3) Mean Absolute Error(MAE) (4) R² or Coefficient of Determination (5) Adjusted R²"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q78",
        "question": "What are MSE and RMSE?",
        "answer": [
            "MSE stands for Mean Square Error while RMSE stands for Root Mean Square Error. They are metrics with which we can evaluate models."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q79",
        "question": "What is the bias-variance trade-off?",
        "answer": [
            "Bias is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. Variance is a model sensitivity to changes in the training dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q80",
        "question": "What is overfitting?",
        "answer": [
            "When your model perform very well on your training set but can't generalize the test set, because it adjusted a lot to the training set."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q81",
        "question": "How to validate your models?",
        "answer": [
            "One of the most common approaches is splitting data into train, validation and test parts. Models are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset. Another approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds. Also you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q82",
        "question": "Why do we need to split our data into three parts: train, validation, and test?",
        "answer": [
            "The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never seen before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q83",
        "question": "Can you explain how cross-validation works?",
        "answer": [
            "Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q84",
        "question": "What is K-fold cross-validation?",
        "answer": [
            "K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set. It should not be used in a time series data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q85",
        "question": "How do we choose K in K-fold cross-validation? What’s your favorite K?",
        "answer": [
            "There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. I tend to use 4 for small datasets and 5 for large ones as K."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q86",
        "question": "What is classification? Which models would you use to solve a classification problem?",
        "answer": [
            "Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q87",
        "question": "What is logistic regression? When do we need to use it?",
        "answer": [
            "Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, spam and not spam, churn and not churn and so on. The variable is said to be a binary or dichotomous."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q88",
        "question": "Is logistic regression a linear model? Why?",
        "answer": [
            "Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q89",
        "question": "What is sigmoid? What does it do?",
        "answer": [
            "A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q90",
        "question": "How do we evaluate classification models?",
        "answer": [
            "Depending on the classification problem, we can use the following evaluation metrics: (1) Accuracy (2) Precision (3) Recall (4) F1 Score (5) Logistic loss (also known as Cross-entropy loss) (6) Jaccard similarity coefficient score"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q91",
        "question": "What is accuracy?",
        "answer": [
            "Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q92",
        "question": "Is accuracy always a good metric?",
        "answer": [
            "Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q93",
        "question": "What is the confusion table? What are the cells in this table?",
        "answer": [
            "Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q94",
        "question": "What are precision, recall, and F1-score?",
        "answer": [
            "Precision and recall are classification evaluation metrics: (1) P = TP / (TP + FP) and R = TP / (TP + FN). (2) Where TP is true positives, FP is false positives and FN is false negatives (3) In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives. (4) F1 is a combination of both precision and recall in one score (harmonic mean): (5) F1 = 2 * PR / (P + R). (6) Max F score is 1 and min is 0, with 1 being the best."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q95",
        "question": "Precision-recall trade-off",
        "answer": [
            "Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. In an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q96",
        "question": "What is the ROC curve? When to use it?",
        "answer": [
            "ROC stands for Receiver Operating Characteristics. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q97",
        "question": "What is AUC (AU ROC)? When to use it?",
        "answer": [
            "AUC stands for Area Under the ROC Curve. ROC is a probability curve and AUC represents degree or measure of separability. It's used when we need to value how much model is capable of distinguishing between classes. The value is between 0 and 1, the higher the better."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q98",
        "question": "How to interpret the AU ROC score?",
        "answer": [
            "AUC score is the value of Area Under the ROC Curve."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q99",
        "question": "What is the PR (precision-recall) curve?",
        "answer": [
            "A precision-recall curve (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q100",
        "question": "What is the area under the PR curve? Is it a useful metric?",
        "answer": [
            "The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q101",
        "question": "In which cases AU PR is better than AU ROC?",
        "answer": [
            "What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR. Typically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q102",
        "question": "What do we do with categorical variables?",
        "answer": [
            "Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including: One-hot encoding. Label encoding. Ordinal encoding. Target encoding"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q103",
        "question": "Why do we need one-hot encoding?",
        "answer": [
            "If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q104",
        "question": "What is curse of dimensionality?",
        "answer": [
            "The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that the curse of dimensionality is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q105",
        "question": "What happens to our linear regression model if we have three columns in our data: x, y, z  —  and z is a sum of x and y?",
        "answer": [
            "We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression  would be a singular (not invertible) matrix."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q106",
        "question": "What is regularization? Why do we need it?",
        "answer": [
            "Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q107",
        "question": "Which regularization techniques do you know?",
        "answer": [
            "There are mainly two types of regularization, L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. Where  determines the amount of regularization."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q108",
        "question": "What kind of regularization techniques are applicable to linear models?",
        "answer": [
            "AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin–Osher–Fatemi model (TV), Potts model, RLAD, Dantzig Selector,SLOPE"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q109",
        "question": "How does L2 regularization look like in a linear model?",
        "answer": [
            "L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q110",
        "question": "How do we select the right regularization parameters?",
        "answer": [
            "Regularization parameters can be chosen using a grid search"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q111",
        "question": "What’s the effect of L2 regularization on the weights of a linear model?",
        "answer": [
            "L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q112",
        "question": "How L1 regularization looks like in a linear model?",
        "answer": [
            "L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q113",
        "question": "What’s the difference between L2 and L1 regularization?",
        "answer": [
            "Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared. Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not. Computational efficiency: L2 has an analytical solution, while L1 does not. Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q114",
        "question": "Can we have both L1 and L2 regularization components in a linear model?",
        "answer": [
            "Yes, elastic net regularization combines L1 and L2 regularization."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q115",
        "question": "What’s the interpretation of the bias term in linear models?",
        "answer": [
            "Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q116",
        "question": "How do we interpret weights in linear models?",
        "answer": [
            "Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds. If the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q117",
        "question": "If a weight for one variable is higher than for another  —  can we say that this variable is more important?",
        "answer": [
            "Yes - if your predictor variables are normalized. Without normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q118",
        "question": "When do we need to perform feature normalization for linear models? When it’s okay not to do it?",
        "answer": [
            "Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. Linear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, — it adds the regularization matrix to the feature matrix before inverting it."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q119",
        "question": "What is feature selection? Why do we need it?",
        "answer": [
            "Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q120",
        "question": "Is feature selection important for linear models?",
        "answer": [
            "Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q121",
        "question": "Which feature selection techniques do you know?",
        "answer": [
            "Here are some of the feature selections: Principal Component Analysis. Neighborhood Component Analysis. ReliefF Algorithm"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q122",
        "question": "Can we use L1 regularization for feature selection?",
        "answer": [
            "Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q123",
        "question": "Can we use L2 regularization for feature selection?",
        "answer": [
            "No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q124",
        "question": "What are the decision trees?",
        "answer": [
            "This is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. A decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable. Various techniques : like Gini, Information Gain, Chi-square, entropy."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q125",
        "question": "How do we train decision trees?",
        "answer": [
            "Start at the root node. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q126",
        "question": "What are the main parameters of the decision tree model?",
        "answer": [
            "(1) maximum tree depth (2) inimum samples per leaf node (3) impurity criterion"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q127",
        "question": "How do we handle categorical variables in decision trees?",
        "answer": [
            "Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q128",
        "question": "What are the benefits of a single decision tree compared to more complex models?",
        "answer": [
            "(1) easy to implement (2) fast training (3) fast inference (4) good explainability"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q129",
        "question": "How can we know which features are more important for the decision tree model?",
        "answer": [
            "Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q130",
        "question": "What is random forest?",
        "answer": [
            "Random Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q131",
        "question": "Why do we need randomization in random forest?",
        "answer": [
            "Random forest in an extension of the bagging algorithm which takes random data samples from the training dataset (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a random sample of m features from full set of n features (with replacement) and uses this subset of features as candidates for the split"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q132",
        "question": "What are the main parameters of the random forest model?",
        "answer": [
            "max_depth: Longest Path between root node and the leaf, min_sample_split: The minimum number of observations needed to split a given node, max_leaf_nodes: Conditions the splitting of the tree and hence, limits the growth of the trees, min_samples_leaf: minimum number of samples in the leaf node, n_estima ors: Number of trees, max_sample: Fraction of original dataset given to any individual tree in the given model, max_features: Limits the maximum number of features provided to trees in random forest mode"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q133",
        "question": "How do we select the depth of the trees in random forest?",
        "answer": [
            "The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting: limit the maximum depth of a tree. limit the number of test nodes. limit the minimum number of objects at a node required to split. do not split a node when, at least, one of the resulting subsample sizes is below a given threshold. stop developing a node if it does not sufficiently improve the fit."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q134",
        "question": "How do we know how many trees we need in random forest?",
        "answer": [
            "The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q135",
        "question": "Is it easy to parallelize training of a random forest model? How can we do it?",
        "answer": [
            "Yes, R provides a simple way to parallelize training of random forests on large scale data. It makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q136",
        "question": "What are the potential problems with many large trees?",
        "answer": [
            "(1) Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn't generalize well to new, unseen data. (2) Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset. (3) Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware. (4) Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important. (5) Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive. It's important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q137",
        "question": "What happens when we have correlated features in our data?",
        "answer": [
            "In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. In general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to do the same job i.e. explain some variance, reduce entropy, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q138",
        "question": "What is gradient boosting trees?",
        "answer": [
            "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q139",
        "question": "What s the difference between random forest and gradient boosting?",
        "answer": [
            "1 Random Forests builds each tree independently while Gradient Boosting builds one tree at a time. 2 Random Forests combine results at the end of the process (by averaging or majority rules) while Gradient Boosting combines results along the way."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q140",
        "question": "Is it possible to parallelize training of a gradient boosting model? How to do it?",
        "answer": [
            "Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost tree_method = 'gpu_hist' option makes training faster by use of GPUs."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q141",
        "question": "What are the main parameters in the gradient boosting model?",
        "answer": [
            "There are many parameters, but below are a few key defaults. 1 learning_rate=0.1 (shrinkage). 2 n_estimators=100 (number of trees). 3 max_depth=3. 4 min_samples_split=2. 5 min_samples_leaf=1. 6 subsample=1.0."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q142",
        "question": "How do you approach tuning parameters in XGBoost or LightGBM?",
        "answer": [
            "Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q143",
        "question": "How do you select the number of trees in the gradient boosting model?",
        "answer": [
            "Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q144",
        "question": "Which hyper-parameter tuning strategies (in general) do you know?",
        "answer": [
            "There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following: 1 Grid Search is an exhaustive approach such that for each hyper-parameter, the user needs to manually give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it's easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge. 2 Random Search is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored. 3 In a completely different framework, Bayesian Optimization is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a simple explanation is that a much simpler/faster acquisition function intelligently chooses (using a surrogate function such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q145",
        "question": "What kind of problems neural nets can solve?",
        "answer": [
            "Neural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q146",
        "question": "How does a usual fully-connected feed-forward neural network work?",
        "answer": [
            "In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q147",
        "question": "Why do we need activation functions?",
        "answer": [
            "The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q148",
        "question": "What are the problems with sigmoid as an activation function?",
        "answer": [
            "The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient — during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q149",
        "question": "What is ReLU? How is it better than sigmoid or tanh?",
        "answer": [
            "ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q150",
        "question": "How we can initialize the weights of a neural network?",
        "answer": [
            "Proper initialization of weight matrix in neural network is very necessary. Simply we can say there are two ways for initializations. 1 Initializing weights with zeroes. Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different. 2 Initializing weights randomly. Assigning random values to weights is better than just 0 assignment. a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time. b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q151",
        "question": "What if we set all the weights of a neural network to 0?",
        "answer": [
            "If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q152",
        "question": "What regularization techniques for neural nets do you know?",
        "answer": [
            "L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. Data Augmentation - This requires some fake data to be created as a part of training set. Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q153",
        "question": "What is dropout? Why is it useful? How does it work?",
        "answer": [
            "Dropout is a technique that at each training step turns off each neuron with a certain probability of p. This way at each iteration we train only 1-p of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter p."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q154",
        "question": "What is backpropagation? How does it work? Why do we need it?",
        "answer": [
            "The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. The weights that minimize the error function is then considered to be a solution to the learning problem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q155",
        "question": "Which optimization techniques for training neural nets do you know?",
        "answer": [
            "Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent(best among gradient descents), Nesterov Accelerated Gradient, Momentum, Adagrad, AdaDelta, Adam(best one. less time, more efficient)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q156",
        "question": "How do we use SGD (stochastic gradient descent) for training a neural net?",
        "answer": [
            "SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q157",
        "question": "What is the learning rate?",
        "answer": [
            "The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the step width during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q158",
        "question": "What happens when the learning rate is too large? Too small?",
        "answer": [
            "A large learning rate can accelerate the training. However, it is possible that we shoot too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won't update even if it is not the best possible global solution."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q159",
        "question": "How to set the learning rate?",
        "answer": [
            "There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn't overshoot or converge too slowly."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q160",
        "question": "What is Adam? What’s the main difference between Adam and SGD?",
        "answer": [
            "Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. Adam tends to converge faster, while SGD often converges to more optimal solutions. SGD's high variance disadvantages gets rectified by Adam (as advantage for Adam)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q161",
        "question": "When would you use Adam and when SGD?",
        "answer": [
            "Adam tends to converge faster, while SGD often converges to more optimal solutions."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q162",
        "question": "Do we want to have a constant learning rate or we better change it throughout training?",
        "answer": [
            "Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don't want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use a function called StepLR which decays the learning rate of each parameter by value gamma-which we have to pass through argument- after n number of epoch which you can also set through function argument named epoch_size."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q163",
        "question": "How do we decide when to stop training a neural net?",
        "answer": [
            "Simply stop training when the validation error is the minimum."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q164",
        "question": "What is model checkpointing?",
        "answer": [
            "Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q165",
        "question": "How we can use neural nets for computer vision?",
        "answer": [
            "Neural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q166",
        "question": "What’s a convolutional layer?",
        "answer": [
            "The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks’ kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q167",
        "question": "Why do we actually need convolutions? Can’t we use fully-connected layers for that?",
        "answer": [
            "A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q168",
        "question": "What’s pooling in CNN? Why do we need it?",
        "answer": [
            "Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q169",
        "question": "How does max pooling work? Are there other pooling techniques?",
        "answer": [
            "Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q170",
        "question": "What are augmentations? Why do we need them?",
        "answer": [
            "Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q171",
        "question": "What kind of augmentations do you know?",
        "answer": [
            "There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q172",
        "question": "How to choose which augmentations to use?",
        "answer": [
            "Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q173",
        "question": "What is transfer learning? How does it work?",
        "answer": [
            "Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ≠ D_T,or T_S ≠ T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks. In the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q174",
        "question": "What is object detection? Do you know any architectures for that?",
        "answer": [
            "Object detection is finding Bounding Boxes around objects in an image. Architectures : YOLO, Faster RCNN, Center Net"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q175",
        "question": "What is object segmentation? Do you know any architectures for that?",
        "answer": [
            "Object Segmentation is predicting masks. It does not differentiate objects. Architectures : Mask RCNN, UNet"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q176",
        "question": "How can we use machine learning for text classification?",
        "answer": [
            "Machine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q177",
        "question": "What is bag of words? How we can use it for text classification?",
        "answer": [
            "Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q178",
        "question": "What are the advantages and disadvantages of bag of words?",
        "answer": [
            "Advantages: 1 Simple to understand and implement. Disadvantages: 1 The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations. 2 Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons. 3 Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”)."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q179",
        "question": "What are N-grams? How can we use them?",
        "answer": [
            "The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q180",
        "question": "What is TF-IDF? How is it useful for text classification?",
        "answer": [
            "Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like “the” that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q181",
        "question": "Which model would you use for text classification with bag of words features?",
        "answer": [
            "1 Bag Of Words model 2 Word2Vec Embeddings 3 fastText Embeddingsn 4 Convolutional Neural Networks (CNN) 5 Long Short-Term Memory (LSTM) 6 Bidirectional Encoder Representations from Transformers (BERT)"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q182",
        "question": "Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words?",
        "answer": [
            "Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q183",
        "question": "What are word embeddings? Why are they useful? Do you know Word2Vec?",
        "answer": [
            "Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations. Word Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q184",
        "question": "If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it?",
        "answer": [
            "Approaches ranked from simple to more complex: 1 Take an average over all words 2 Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf). 3 Use ML model like LSTM or Transformer."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q185",
        "question": "What is unsupervised learning?",
        "answer": [
            "Unsupervised learning aims to detect patterns in data where no labels are given."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q186",
        "question": "What is clustering? When do we need it?",
        "answer": [
            "Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q187",
        "question": "Do you know how K-means works?",
        "answer": [
            "1 Partition points into k subsets. 2 Compute the seed points as the new centroids of the clusters of the current partitioning. 3 Assign each point to the cluster with the nearest seed point. 4 Go back to step 2 or stop when the assignment does not change."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q188",
        "question": "How to select K for K-means?",
        "answer": [
            "Domain knowledge, i.e. an expert knows the value of k Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters. Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q189",
        "question": "What are the other clustering algorithms do you know?",
        "answer": [
            "k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise. Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster. DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster. Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q190",
        "question": "Do you know how DBScan works?",
        "answer": [
            "Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood) Cluster defined as maximum set of density-connected points. Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts. p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1. p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q191",
        "question": "When would you choose K-means and when DBScan?",
        "answer": [
            "DBScan is more robust to noise. DBScan is better when the amount of clusters is difficult to guess. K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q192",
        "question": "What is the curse of dimensionality? Why do we care about it?",
        "answer": [
            "Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q193",
        "question": "Do you know any dimensionality reduction techniques?",
        "answer": [
            "Singular Value Decomposition (SVD), Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), T-distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders, Fourier and Wavelet Transforms"
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q194",
        "question": "What’s singular value decomposition? How is it typically used for machine learning?",
        "answer": [
            "Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Σ (diagonal matrix) and R^T (right singular values). For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive. Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q195",
        "question": "What is precision and recall at k?",
        "answer": [
            "Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first k results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top k results out of all correct answer for a given query."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q196",
        "question": "What is a recommender system?",
        "answer": [
            "Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q197",
        "question": "What are good baselines when building a recommender system?",
        "answer": [
            "A good recommer system should give relevant and personalized information. It should not recommend items the user knows well or finds easily. It should make diverse suggestions. A user should explore new items."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q198",
        "question": "What is collaborative filtering?",
        "answer": [
            "Collaborative filtering is the most prominent approach to generate recommendations. It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others. A recommendation is calculated as the average of other experiences. Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi. In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating. Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases. Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q199",
        "question": "How we can incorporate implicit feedback (clicks, etc) into our recommender systems?",
        "answer": [
            "In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q200",
        "question": "What is the cold start problem?",
        "answer": [
            "Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    },
    {
        "id": "Q201",
        "question": "What is the cold start problem?",
        "answer": [
            "Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user."
        ],
        "tags": [
            "statistics",
            "data science",
            "interview"
        ]
    }
]